
# 介绍


- 神经网络:  
	[机器学习-神经网络](机器学习.md#神经网络)  

深度学习(Deep Learning). 随着神经网络不断发展, 人们将其从机器学习中独立出来, 成为一种单独的算法分支--深度学习.  

随着数据量的增加, 人们能够构建规模更大的神经网络, 取得的效果也越好, 至此, 深度学习也就兴盛起来.  


## 符号说明

特征向量: n_x.  
样本矩阵(m个样本): X,  n_x * m 或 m * n_x.  
样本标签: Y, 1 * m 的向量.  


**以逻辑回归为例**  

输出(预测值): $\hat{y} = P(y=1\mid x) = \sigma(w^T x + b)$  
σ: sigmoid函数, $\sigma(z) = \frac{1}{1+e^{-z}}$  
w, b: 神经网络中的参数, 这里将其从θ中分离成两个独立的参数, b 为偏置项(常数项).  

$u = w^T x$  
$v = u + b$  

损失函数(cost):  $\log 默认为 \ln$  
$$J(\omega , b) = - \frac{1}{m} \sum^m_{i=1} \left[ y^i \log \hat{y^i} + (1-y^i) \log(1-\hat{y^i}) \right]$$
$$\frac{dJ}{d\hat{y}} = - \frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}$$ 
梯度下降:  
$w := w - \alpha \frac{dJ(w)}{dw}$

反向传播:  
$d_{var}$ : 损失函数 J 对中间变量的偏导.  

神经网络:  
L: 神经网络层数, 不包括输入层.  
$n^{[l]}$ : 第 $l$ 层神经网络.
$a^{[l]}$ : 第 $l$ 层的激活函数.  


## 向量化

在神经网络的计算中一律使用 向量/矩阵, 而不使用 for循环, 各计算库都对 矩阵计算 进行了优化, 可以更好地利用 CPU/GPU 的并行能力, 极大地提升计算速度.  


# 神经网络

$a^{[l]}_{i}$ : 第 l 层, 第 i 个节点.  
每个节点都会计算: $z = w^T x + b$ , $a = \sigma(z)$ 

对于每一层隐藏层来说:  
$Z = W^T X + B$ , $A = \sigma(Z)$  
W: n * n 矩阵; X: n * m 矩阵; B: 1 * n 向量, n 等于上一层的 节点/输入 的数量.  


### 激活函数

激活函数是一个非线性函数, 用 g(z) 表示.  
sigmoid函数: $\sigma(z) = \frac{1}{1 + e^{-z}}$ $\sigma'(z) = a(1-a)$  
tanh函数: $tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ $tanh'(z) = 1 - a^2$  

tanh函数几乎在所有场合都比 sigmoid函数更优秀. sigmoid函数一般只作为二元分类输出层的激活函数.   

上面两个函数都有一个问题, 当 z 非常大/小 时, 其导数接近 0, 这会使梯度下降变得非常缓慢.  

线性修正单元:  $a = max(0, z)$  
$$
ReUL(z) = \left \{
\begin{aligned}
	0 \quad if  \quad z \lt 0 \\
	z \quad if  \quad z \geq 0 \\
\end{aligned}
\right.
$$
一般默认使用 ReUL.  

- 使用激活函数的必要.  
	当不使用或者其为线性时:
	$a_1 = w_1 x_1 + b_1$  
	$a_2 = w_2 a_1 + b_2 = w_2 w_1 x_1 + w_2 b_1 + b_2$   
	w_2 与 w_1 * w_2 并没有什么区别, 常数项也一样.  
	无论多少层隐藏层都和一层没有区别, 或者没有隐藏层.  


### 反向传播

通过正向传播可以计算损失(代价), 通过反向传播的链式求导法则可以计算神经网络中各参数的导数, 来实现梯度下降.  

- 正向传播:  
	$z = w^T x + b$  
	$a = g(z)$  
	$J(W, B) = \frac{1}{m} \sum^m_{i=1} l(\hat{y}, y)$  
	
- 反向传播:  
	J()为逻辑回归的代价函数.  
	$g(z) = sigmoid$  
	输出层: $dZ = \hat{Y} - Y$  
	隐藏层: $(dZ)^{n-1} = (W^n)^T (dZ)^{n} * g'(z)^{n-1}$  
	$dW = \frac{1}{m} dZ \ X^T$  
	$dB = \frac{1}{m} np.sum(dZ, axis = 1, keepdims=True)$  


### 随机初始化

偏置项 b 全部初始化为 0, 没有任何问题.  
但如果 w 全部为 0, 则节点会完全对称(X矩阵每一列都相同), 迭代时梯度下降也会完全对称($dW = \frac{1}{m} dZ \ X^T$).  
所以 W 会随机初始化为在 0 附近的值(0+-0.03). 接近 0 是因为这样经过激活函数后仍接近 0, 此时激活函数的梯度(导数)较大, 下降比较快.  


### 超参数

一般的参数指各层网络中的 W, b.  
其他的例如, 学习率α, 迭代次数, 隐藏层层数... 这些参数在一定程度上会影响最终的 W,b 的值, 称为**超参数**.  

超参数的值需要人为设定, 且难以一开始就知道其最优值, 需要不断测试并进行修改.  



# 训练

## 数据集

数据集会被分成 训练集, 交叉验证集, 测试集. 不同数据集应满足同一分布.  
训练集: 用来迭代参数.  
(交叉)验证集: 检测哪种算法更有效.  
测试集: 测试模型的效果.  

在机器学习的时代, 其分割比例一般为 7:3:0, 6:2:2.  
在大数据时代, 数据量变得非常庞大(百万), 可以将数据更多地分配在训练集上, 其比例可能是 98:1:1 或是 99.5 : 0.25 : 0.25.  


## 偏差与方差

[机器学习-模型诊断](机器学习.md#模型诊断)

- 欠拟合:  
	对训练集和测试集的数据预测准确率都不高, 即高偏差.  

- 过拟合:  
	对训练集的数据拟合非常好, 准确率非常高, 但对测试集数据的准确率非常低, 即高方差.  

高偏差与高方差是可能同时出现的, 既欠拟合又过拟合.  
高偏差与高方差是个相对的概念, 并没有绝对的分界线区分是否为高偏差或是否为高方差.  
![高偏差高方差](https://pic.imgdb.cn/item/64c1fc7e1ddac507ccab78d2.jpg)

### 正则化

正则化是解决过拟合问题的主要方法.  

$$J(w,b) = \frac{1}{m} \sum^m_{i=1} l(\hat{y},y) + \frac{\lambda}{2m} \| w \|^2$$
代价函数后加上的 $\frac{\lambda}{2m} \|w\|^2$ 为**正则项**(l_2范数).  
$\|w\|^2 = w^T w$  

加上后, 会让参数尽量小, 且比较接近, 这样函数会更加平滑. 但λ的值过大会让模型欠拟合.  

- dropout正则:  
	dropout正则每次训练会随机使神经网络中的节点失效, 仅使用剩下的节点进行计算和梯度下降.  
	
	`d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep-prob`  
	> a3: 第3层的激活值.  
	> keep-prob: 保留概率, (0, 1).  
	> d3: bool索引, 进行逻辑判断后对应的节点列表.  

	`a3 = np.multiply(a3, d3)`  
	> a3 与 d3 相乘, 可以去除判断为 false 的节点, 使其为 0.  

	`a3 /= keep-prob`  
	> 保证 a3 的期望值不变.  

	过拟合是样本数少或特征数多造成的. 此方法也是在减小神经网络的复杂程度, 会将权重分散, 使模型更好地拟合样本数据.  	


## 归一化处理

计算数据均值: $\mu = \sum^m_{i=1} x^i$.  
数据都用均值表示: $x^i -= \mu$.  
归一化: $\sigma^2 = \frac{1}{m} \sum^m_{i=1} (x^i)^2$  $x^i /= \sigma^2$

这样就可以控制数据的均值和方差, 比较规则的数据能使代价函数更快收敛.  


## 梯度

### 梯度消失与梯度爆炸

在反向传播过程中:  
$(dZ)^{n-1} = (W^n)^T (dZ)^{n} * g'(z)^{n-1}$  

当神经网络层数较多时,  
g'(z) < 1, 其导数值会逐渐减小.  
g'(z) > 1, 其导数值会逐渐增大.  

$z = \sum^m_{i=1} x^i w^i$  
w 初始化应在一个合理的区间.  
`w = np.random.rand(shap) * np.sqrt(1/n)`  
如果激活函数为 ReUL, 换成 2/n 会有更好的效果. n 为上一层神经元数量.  
w 会与特征向量处于相似的分布中.  


### 梯度检测

对于神经网络中的每个参数进行梯度检测

导数计算公式:  
$$f'(x) = \frac{f(x+\epsilon) - f(x - \epsilon)}{2\epsilon}$$

梯度计算:
$$
d\Theta^{[i]}_{approx} = \frac{J(\theta_{i} + \epsilon) - J(\theta_{i} - \epsilon)}{2\epsilon}
$$
梯度检测:  
$$
check = \frac{\| d\Theta_{approx} - d\theta \|}{\| d\Theta_{approx} \| + \| d\theta \|}
$$
> 其值应小于 10^(-3), 一般在 10^(-7).  
> dθ为反向传播过程中计算出的梯度值.  

一般在训练过程中不会进行梯度检测, 其计算非常慢, 只在调试的时候用于检测.  
dropout正则 与 梯度检测不兼容.  


## 超参数的调节

- 目前的超参数:  
	- 学习率: α.  
	- 动量: β, β_1, β_2, ε.  
	- 隐藏层层数: layers.  
	- 隐藏层单元数: hidden units.  
	- 学习率衰减: learning rate decay.  
	- 数据组大小: mini-batch size.  

- 最重要的为 α.  
- 其次为 β, 隐藏层单元数, mini-batch大小.  
- 然后是 隐藏层数, 学习衰减率.  
- 最后 动量其他部分一般固定为 0.9, 0.999, 10^-8.  


- 调整过程:  
	- 在一定范围内随机取值.  
	- 确定效果比较好的各参数范围.  
	- 缩小取值范围.  
	- 继续随机取值.  
> 有时不希望随机的概率为均匀分布(参数对结果的影响非线性), 可以对随机数再用对数或指数, 即对随机值进行各种后期处理.  


- 可以根据不同的资源采用不同的调节方案:  
	- 当计算机无法承担多个模型的训练时:  
		可以边训练边适当调节参数.  
	- 当计算机资源充足时:  
		可以同时训练多个不同的模型, 来对比其效果.  




# 优化

## mini-batch

在向量化计算过程中, 样本数据为 n x m 的矩阵.  
当 m 过大时(百万), 计算速度仍然很慢, 负担较大.  

此时, 可以将训练数据分成小块, 每个batch有合适的训练数据量.  
mini-batch_size 的选择, 其应在 1 至 m 之间. 越大迭代越缓慢, 越小下降方向越不稳定.  

![迭代对比](https://pic.imgdb.cn/item/64c865721ddac507cc20f930.png)


- 学习率衰减
	因为其梯度下降方向不一定是最低点, 其最后可能会在最低点附近不断波动, 难以收敛.  
	可以通过减小学习率来使其更容易收敛, 即学习率衰减.  
	α = α_0 / (1 + decay-rate * epoch-num)  
	> α_0 : 初始学习率.  
	> decay-rate : 衰减率.  
	> epoch-num : 迭代数.  
	
	也有一些其他的衰减公式...  


## 指数加权平均*

以指数式递减加权的移动平均 来计算一组数据的平均值.  
逐个计算前n个数据的带权平均值, 越靠后的数据加权越重, 靠前数据的权重指数递减.  

$v_{i} = \beta \ v_{i-1} + (1-\beta)x_{i}$  
> v : 平均值  
> x : 数据  
> β: 参数, 取 0.9  

其可视为计入最近 $\frac{1}{1-\beta}$ 个数据的值, 其权重约为 1/e.  

![指数加权移动平均](https://pic.imgdb.cn/item/64c865791ddac507cc2105fd.png)


β值影响的是"过去"与"现在"的权重. 其增大时, 过去的权重增大, 变化会"延迟"; 减小时, 现在的权重增大, 变化更加敏感.  
![更改β](https://pic.imgdb.cn/item/64c865801ddac507cc2110ac.png)


- 偏差修正:  
	当 $v_0$ 初始化为 0 时, 由于其之前的数据被视为0, 所以靠前的计算值会非常低, 而不是像上图一样接近数据的均值. 所以一般需要有偏差修正.  
	$v_i = v_{i}/(1 - \beta^i)$, β^i 随 i 增加而逐渐接近 0.  


## 梯度下降中的摆动


由于特征向量各维度的初始值与梯度不同, 而它拥有相同的学习率. 容易使得一边学习快, 在最小值附近摆动; 另一边学习慢.  

![摆动梯度下降](https://pic.imgdb.cn/item/64c8a48d1ddac507cca17efe.png)

减小摆动的幅度可以让其往更接近最小值的方向前进, 使损失函数更快收敛.  

- 动量梯度下降法

	计算出 dw, db 后:  
	V_dw = β V_dw + (1-β)dw  
	V_db = β V_db + (1-β)db 
	> β: 动量(momentum).  
	> (1-β) 有时会被删去, 仅影响最佳的 α 值  
	
	w -= αV_dw  
	b -= αV_db  
	如果梯度在正负值摇摆, 其叠加值(V)会趋近 0, 如果为单一正负, 其获得加速.  


- RMSprop  

	S_dw = β S_dw + (1-β)(dw)^2  
	S_db = β S_db + (1-β)(db)^2  
	$W = W - α \ dw/\sqrt{S_{dw}}$  
	$b = b - α \ db/\sqrt{S_{db}}$  


- Adam优化算法

	Adam算法结合了**动量梯度下降**和**RMSprop**.  

	步骤:  
	初始化 V_dw = 0, S_dw = 0, V_db = 0, S_db = 0.  
	计算出 dw, db.    
	$V_{dw} = \beta_{1} V_{dw} + (1-\beta_{1}) dw$  
	$V_{db} = \beta_{1} V_{db} + (1-\beta_{1}) db$  
	$S_{dw} = \beta_{2}S_{dw} + (1-\beta_{2})(dw)^2$  
	$S_{db} = \beta_{2} S_{db} + (1-\beta_{2})(db)^2$  	

	$V_{dw} = V_{dw}/(1-(\beta_{1})^t)$, $V_{db} = V_{db}/(1-(\beta_{1})^t)$.  
	$S_{dw} = S_{dw}/(1-(\beta_{2})^t)$, $S_{db} = S_{db}/(1-(\beta_{2})^t)$.  

	$w = w - \alpha V_{dw} / \sqrt{ S_{dw} + \epsilon }$, $b = b - \alpha V_{db} / \sqrt{ S_{db} + \epsilon }$.  
	加上 $\epsilon$ 是为了避免分母为 0, 其值一般非常小.  

	各超参数值的选择(推荐值):  
	α: 需要不断调整.  
	$\beta_1$ : 0.9  
	$\beta_2$ : 0.999  
	$\epsilon$ : 10^(-8), 影响不大.  


## 局部最优问题

在说到局部最优问题时, 一般想的可能是这样的:  
![局部最优](https://pic.imgdb.cn/item/64cb65f71ddac507ccb71ade.png)
但在神经网络中, 零梯度点往往是鞍点.  
在几千上万维度的空间中, 一个点所有维度的梯度都为零的概率非常小. 所以更可能碰到鞍点而不是局部最优.  
![鞍点](https://pic.imgdb.cn/item/64cb681d1ddac507ccbc5e5e.png)

在梯度下降过程中可能会遇到"平稳段", 此时梯度趋近零, 学习速度非常慢, 用Adam算法也许会有改善(可以加速).  




## batch 归一化

如果对输入参数进行归一化处理, 可以使梯度下降进行得更顺利. 但对于多层的神经网络, 只有第一层的输入是归一化的.  
那是否可以对每一层的输入都进行归一化呢?  


- 对于每层隐藏层:  
	计算出所有z值(激活函数之前的值), $z^1, \dots, z^m$.  
	计算均值: $\mu = \frac{1}{m} \sum_{i} z^i$  
	计算方差: $\sigma^2 = \frac{1}{m} \sum_{i} (z^i - \mu)^2$  
	标准化正态分布: $Z^i_{norm} = \frac{z^i \mu}{\sqrt{ \sigma^2 + \epsilon }}$  
	其他正态分布: $\tilde{Z}^i = \gamma Z^i_{norm} + \beta$, β为均值, γ为方差.  
	因为一些激活函数(如 sigmoid), 其 0 附近为线性部分, 所以需要改变输入的分布.  

	现在, 可以用 $\tilde{Z}^i$ 代替 $Z^i$,  


- 在使用batch归一化后:  
$$X \stackrel{w, b}{\to} Z \stackrel{\beta,\gamma}{\to} \tilde{Z} \to g(\tilde{Z}) = a$$
> $\beta, \gamma$ : 和其他参数一样可以根据梯度进行下降学习.  
> 由于进行了归一化, 常数项参数 b 对于输出没有作用, 可以设为0, 或直接去掉.  


- 作用:  
	- 如果训练集与测试集的分布不一致, 往往需要重新训练整个神经网络. 因为每一层的输入输出相互关联. batch归一化后, 输入(输出)的分布前后保持一致, 可以更快地进行学习.  


- 在 mini-bitch 中的应用:  
	由于每个 mini-bitch 的均值/方差不尽相同, 与整体的均值/方差不一致, 在处理时会用指数加权平均来代表整体的均值/方差.  


# Softmax回归

属于逻辑回归的多元分类.  

如果需要分n类, 输出应为n维向量, 表示分成各类的概率.  
如分成四类(A类, B类, C类, 其他/Other), $\hat{h} = [P(A), P(B), P(C), P(O)]$  

Softmax激活函数:  
在输出层时, 计算:  
$t = e^z$  
$a = \frac{t}{\sum^n_{i=0} t_{i}}$  
> z, t, a 应为 n 维向量.  

也就是从数值转化为百分比, 但不是线性的, 可以处理负数部分.  

一般种类数量会用 C 表示.  

- 训练:
	- 损失函数: $l(\hat{y}, y) = - \sum_{j=0}^C y_{j} \log(\hat{y}_{j})$  



# 深度学习框架

深度学习的框架每天都在进步, 框架的推荐需要实时从网络上浏览, 先介绍选择框架的标准.  

- 便于编程.  
- 运算速度.  
- 真正开放(开源+良好的管理).  


框架:  
- TensorFlow:  
	由谷歌维护, 目前主流的深度学习框架, 入门难度较大.  

- Keras:  
	对TensorFlow的高级集成API, 对小白友好易入门的框架.  

- Caffe:  
	...

- PyTorch:  
	Facebook团队分布的框架(2017), 近年来的关注度不断上升.  



# 策略

对于模型的效果, 当其表现不同时, 需要采取不同的策略来进行调整.  


## 准确率与召回率


$准确率(P) = \frac{预测为真 \& 实际为真}{预测为真}$  

$召回率(R) = \frac{预测为真 \& 实际为真}{实际为真}$  

[不对称性分类的误差评估](机器学习.md#不对称性分类的误差评估)  

F1 分数:  
	F1 分数结合了 查准率(P)与 查全率(R), 作为评判标准.  
	调和平均数: $F1 = 2/ (\frac{1}{R}+\frac{1}{P})$  


如果有多个指标, 而不同模型对不同指标有高有低, 很难从多个指标中筛选最好的模型, 最好使用单一的实数(结合多个指标)来作为一个评判标准.  


## 标注错误

深度学习算法对随机错误很健壮, 但对系统性误差不健壮.  
即小部分的随机标注错误不影响模型的训练.  

系统性的错误可以在错误分析中, 显示其标签, 查看因标签错误的占比, 占比较大时就有必要去修正.  



## 迁移学习

如果已经训练好了一个猫类图片识别模型, 也可以更改输出层, 然后直接训练成其他图片识别分类模型. 因为在先前的训练中, 模型可能已经学会了一些图片识别的基本特征, 这样可以加速其他模型的训练.  
尤其是大模型迁移至小模型时, 已经积累的训练量可能非常有用.  


## 多任务学习

如果需要在一张图片中识别多个目标, 那么其输出为一个向量, 表示是否存在指定的目标.  
虽然可以训练多个模型来分别判断是否存在这些目标, 但其任务十分相似, 可以由一个稍大些的模型一起承担.  

其与多分类模型有些区别, 多分类会在目标中选择一个, 但多任务每个都是单独判断的.  



## 端到端的深度学习

即输入端到输出端.  

如果有一个语音识别任务, 可以先将音频拆分为音节, 然后再转换为文本. 但在训练数据足够和网络够大的情况下, 直接输入音频, 输出文本的模型效果要更好.  

在另一些情况下, 如人脸识别, 更推荐拆分任务: 先定位人脸位置, 并进行合适的缩放, 再进行识别.  

端到端的学习对数据量有非常大要求, 当确信人为地设计流程是有必要且有效的时候, 也不必用端到端的学习方法.  


# 计算机视觉

## 卷积

卷积常用来作简单的图像处理, 如垂直边缘检测, 其为一个 3 x 3 矩阵. 也被称为过滤器, 卷积核.  
图像为三维矩阵, 灰度图像为二维.  

- 卷积运算:  
	将卷积核覆盖于图像上, 作乘算, 得到一个数值, 再不断移动进行乘算, 即可得到一个比原图像小一圈的矩阵.  

![卷积运算](https://pic.imgdb.cn/item/64e82e7c661c6c8e54e5fb6a.gif)  


- 垂直边缘检测:  
	3 种卷积核:  

| 1   | 0   | -1  |     | 1   | 0   | -1  |     | 3   | 0   | -3  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1   | 0   | -1  |     | 2   | 0   | -2  |     | 10  | 0   | -10 |
| 1   | 0   | -1  |     | 1   | 0   | -1  |     | 3   | 0   | -3  | 

> 如果两边颜色相近, 计算结果会趋近 0, 呈深色.  
> 如果两边颜色不同(垂直边缘), 计算结果较大(取绝对值), 呈白色.  
> 通过调整数值也可以得到不同角度的边缘检测, 或是其他过滤器.  

- padding:  
	使用过滤器之后, 图像会缩小.  
	原图像大小: n x n, 过滤器大小: f x f.  
	计算的图像大小: (n - f + 1) x (n - f + 1).  

	对于角落, 边缘的像素的使用次数会比中间的像素更少.  

	为了解决这些问题, 可以进行 padding(扩充).  
	在图像外围填充值为 0 的像素点(p层).  
	扩充后图像边长为: n + 2p.  
	如果想过滤后大小与原来一致, p = (f - 1)/2.  

- 卷积步长:  
	指卷积核在图像上移动的步长, 上图演示的步长即为 1.  
	当步长为 s 时, 输出的图像边长为: (n + 2p - f)/s + 1.  
	如果不是整数, 则向下取整.  

- 三维卷积:  
	图像的第三维为颜色通道(channel), 卷积核的第三维长度(深度)需要与其一致.  
	卷积核的移动与二维时一致, 得到的结果为二维矩阵. 
	过滤器可以有多个, 多个过滤器得到的结果可以叠加在一起, 计算结果的第三维程度等于过滤器个数.  


## 单层卷积网络

三维图像数据通过不同卷积核得到多个二维矩阵.  
矩阵加常数后的结果再通过激活函数即为卷积层输出.  

在非卷积网络中:  
$z = wa + b$  
$a = g(z)$  

在卷积层中:  
卷积核就相当于参数w.  

- 符号:  
	f: 过滤器大小(边长).  
	p: 图片扩充层数.  
	s: 步长.  
	n_H, n_W, n_C: 高, 宽, 通道数.  
	输出的通道数等于过滤器的个数.  
	卷积层的过滤器通道数等于输入的通道数.  


单层的卷积层以此不断叠加形成卷积网络.  
最后的输出放入Softmax回归函数中.  

虽然仅用卷积层也可能构建出很好的神经网络, 但一般依然会增加 **池化层(Pooling/Pool)**, **全连接层(Fully connected/FC)**.  


## 池化层

池化层用来减小模型大小, 提高计算速度, 提高特征提取的健壮性.  
相邻像素为一块, 成为一个整体, 像素值选取最大值.  
简单来说, 即将 "高清图片" 压缩为 "像素图片".  

其与卷积一样可以选取合并(过滤器)的边长, 步长. 但其"计算"方式不同.  
输入输出的通道数量保持一致.  

- 输出像素值:  
	其值也可以选取中位数, 最小值, 平均值, 但相邻像素一般差距不大, 选最大值即可.  

- 过滤器大小与步长:  
	一般: f = 2, s = 2.  

一般会在一个卷积层后连接一个池化层, 两个加起来算作一整层.  


## 全连接层

在卷积层和池化层之后, 将结果化为一维向量, 接入的普通隐藏层即全连接层. 也会有多层. 最后再连接Softmax.  

网络中的大部分参数都会集中在全连接层中.  



# 视觉神经网络模型

## 经典网络结构

### LeNet-5

LeNet-5 是针对灰度图像训练的(手写体字符识别).  

结构:  
输入(32x32x1) -> 卷积层(f=5,s=1,n=6) -> (28x28x6) -> 池化层(14x14x6) -> 卷积层(f=5,s=1,n=16) -> (10x10x16) -> 池化层(5x5x16) -> 全连接层(120) -> 全连接层(84) -> 输出  

随着卷积与池化, 图片大小不断减小, 通道数量不断增加.  
在当时的计算机性能下, 每个过滤器对应其输入的不同通道, 现在一般不这样用.   


### AlexNet

输入:  
227x227x3

C1:  
f = 11, s = 4, n = 96.  
-> 55x55x96

MaxPool:  
f = 3, s = 2.  
-> 27x27x96

C2:  
f = 5, p = 2, s = 1, n = 256.  
-> 27x27x256

MaxPool:  
f = 3, s = 2.  
-> 13x13x256

卷积:  
... -> 13x13x384 -> 13x13x384 -> 13x13x256

MaxPool:  
f = 3, s = 2.  
-> 6x6x256

全连接:  
9216 (fc)-> 4096 (fc)-> 4096 -> SoftMax(1000)  


### VGG-16

输入:  
224x224x3

Conv:  
f = 3, s = 1, p = 1.  

Pool:  
f = 2, s = 2.  

224x224x3 (Covn)-> 224x224x64 (Pool)-> 112x112x64 (Covn)-> 112x112x128 (Pool)-> 56x56x128 (Covn)-> 56x56x256 (Pool)-> 28x28x256 (Covn)-> 28x28x512 (Pool)-> 14x14x512 (Covn)-> 14x14x512 (Pool)->  7x7x512 (FC)-> 4096 (FC)-> 4096 



## 残差网络


当神经网络的层数增多时, 容易引起梯度消失/爆炸的问题.  
也容易出现退化的问题, 随着层数的增加, 训练集的准确率却下降了.  
原因可能是随着深度的增加, 训练难度也增加, 导致训练效果不佳.  

![][D:\word文档\MarkDown文档\image\机器学习\网络退化.png] 

### 残差块:  

Residual block.  

在一般的前向传播过程中:  
$z = wa + b$  
$a = g(z)$  
这称为"主路径".  

捷径(远跳连接):  
前面的激活值会参与后面激活值的计算.  
$a^{l+2} = g(z^{l+2} + a^{l})$  

一个残差网络由许多残差块构成.  
![][D:\word文档\MarkDown文档\image\机器学习\残差网络.png]


