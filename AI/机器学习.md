**机器学习(Machine Learning)**

机器学习是从数据中自动分析获得模型, 并利用模型对未知数据进行预测.  

- 机器学习是人工智能的一个实现途径.  
- 深度学习是机器学习的一个方法发展而来.  

# 介绍

## 机器学习工作流程

1. 获得数据  
一行数据称为一个样本, 一列称为一个特征, 有些数据有目标值(标签值), 有些没有.数据一般分出20%~30%用于测试.  

2. 数据基本处理  
对数据进行缺失值,去除异常值等处理.  

3. 特征工程  
(专业的数据处理, 机器对特征的识别直接影响机器学习的效果以及上限), 包括:  
    - 特征提取
    - 特征预处理
    - 特征降维

4. 机器学习(模型训练)  
5. 模型评估  

## 人工智能主要分支

- 计算机视觉
- 语音识别
- 文本挖掘/分类
- 机器翻译
- 机器人

## 机器学习算法

- 监督学习
	输入数据由输入特征值和目标值组成, 输出可以是一个连续的值(回归), 或是有限个离散的值(分类).  

- 无监督学习
    输入数据由特征值组成, 没有目标值.

- 半监督学习
- 强化学习 

## 模型评估

### 分类模型评估

准确率, 精确率, 召回率, F1-score, AUC指标等.  
主要学习准确率.  

### 回归模型评估

均方根误差(RMSE)是一个衡量回归模型误差率的常用格式.

### 拟合

- 欠拟合: 模型学习过于粗糙, 对数据特征关系要求过低.
- 过拟合: 对数据特征要求过高

## 深度学习

深度学习也称为深度结构学习, 层次学习, 深度机器学习. 基于神经网络发展而来...  


# 基础理论


## 线性回归

通过离散的点(样本)来尽可能还原最符合的函数.  


### 代价函数

也称损失函数, 误差函数.  
通过数据集来得到模型, 为了测量(评估)模型是否拟合数据, 以代价函数作为衡量标准.  

在线性回归问题中, 代价函数一般为方差:  

$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}(h_θ(x_i)-y_i)^2 $$

> m为数据集的大小, $h_{\theta}()$为假设模型(回归函数, 比如 y=θ1x+θ2).  

在线性回归问题中用待定系数法确定模型的系数, 通过代价函数将问题转换为求各不同系数下代价函数的最小值, 以此确定系数.  

当模型(回归函数)中的系数取不同值时, 会得到不同的方差, 构成自变量为回归函数系数, 因变量为对应方差的函数. 根据此函数得到方差最小值时, 此时的系数(自变量)即模型最拟合的系数取值.  

代价函数一般会选择凹函数(局部最小为全局最小), 避免选择的局部最小与全局最小差距过大, 模型不合理.  

### 梯度下降算法

如何计算代价函数的最小值 - 最经典的方案当然是求导.  
梯度下降算法即用来计算代价函数的导数, 并以此修改参数θ.  
此算法将从系数的某个取值(一般全赋0)开始, 将θ不断朝导数为负的方向移动, 直到(区域)最低处.  

$temp_0 := θ_0 - α \frac{d}{dθ_0}J(θ_0, θ_1)$  
$temp_1 := θ_1 - α \frac{d}{dθ_1}J(θ_0, θ_1)$  
$θ_0 := temp_0$  
$θ_1 := temp_1$  

> α为学习率, 决定每次梯度下降的幅度(跨步的大小).  
> 当导数值(偏导) $\frac{d}{d\theta_j}J(\theta)$ 越大时, 下降的幅度自然越大, 所以α一般为常数(固定值).  

![梯度下降-1](https://pic.imgdb.cn/item/64bc9edb1ddac507cc5b3d51.jpg)
![梯度下降-2](https://pic.imgdb.cn/item/64bca4511ddac507cc682a08.jpg)

### 多变量线性回归

通常用多维向量来表示多元变量, 回归函数就变成:  
$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + ... +θ_nx_n$.  
变量θ, x用向量表示:  
$h_θ(x) = θ^TX$, (θ, X为n维向量)


### 多元梯度下降法

与单变量类似, 导数为每个θ变量的偏导, 不断更新θ向量.  


#### 特征收缩

在多个变量的梯度下降中, 由于变量的取值范围差异较大, 导致下降时的路线会非常曲折, 效率低下. 因此需要通过缩放来使变量范围大致相同, 一般在 0~1, -1~1, 0~3之类.  


#### 学习率

在梯度下降中, 每次下降(迭代)都会使 θ 接近最小值, 随迭代此数不断下降.  
以迭代次数为自变量, θ高度为因变量, 会得到类似 y = 1/x 的函数, 即 θ 随迭代次数增加会收敛.  
可以通过绘制这个函数来判断梯度下降算法是否收敛.  

自动收敛测试:  
如果代价函数J(θ)进一步迭代后的下降小于一个很小的值(如e^-3), 就判断函数已经收敛.  
但要设定一个合适的阈值比较困难, 更建议看曲线图.  

也可以通过曲线图判断梯度下降算法是否正常工作(α过大, 或程序错误之类).  


### 正规方程

在求回归函数时, 之前时是通过不断迭代求θ向量, 直到代价函数最小值. 但可以通过运算来直接求代价函数最小值的位置.  

正规方程:  
$θ = (X^T X)^{-1} X^T y$  
> X为特征矩阵, y为目标向量, X一般会在第一列加全为1的一列作为常数项.  
> $(X^T X)$ 是为了得到方阵, 方阵才有逆矩阵.  
> 计算时间(求解θ)一般为O(n^3), n为X的维度.  
> 目前的计算机在n > 10000时不建议使用正规方程, 而使用梯度下降.


#### 矩阵不可逆时求解

一般的特征矩阵不会出现不可逆的情况, 特征之间一般是线性无关的.  
即使矩阵不可逆, pinv()函数也可以通过伪逆矩阵求解.  

原因:  
- 在特征值中有多余的特征变量(或者重复之类的).
- 特征变量过多而样本太少.


## 分类

### 二(元)分类

#### 逻辑回归(logistic regression)

"逻辑"回归: 用来进行分类的算法.  

对于求解分类的函数(假设函数):  
$h_θ(x) = g(θ^T x)$  
$g(z) = \frac{1}{1 + e^{-z}}$  
    
    函数g(z) (Sigmoid函数) 能将结果控制在 0~1 之间. 或是将其转化为概率.
    
    通过训练后能得到模型θ向量, 与特征变量构成一个界限(坐标中的几何曲线或面)  
    这曲线就是决策界限(分类界线), 将其与g(z)复合, 就能得到0或1的分类判断.  

![Sigmoid函数](https://pic.imgdb.cn/item/64bca7831ddac507cc703869.jpg)

#### 代价函数

代价函数可以写成:  
$$J(θ) = \frac{1}{m} \sum^{m}_{i=1} Cost()$$

$$ Cost(h_θ(x), y) = \left\{
\begin{aligned}
    -log(h_θ(x)) \quad if & & y = 1  \\
    -log(1 - h_θ(x)) \quad if & & y = 0 \\
\end{aligned}
\right.
$$
> 预测值 $h_θ(x)$ 在 0-1 内, x_1 ··· x_m.  
> 如果预测值与其类别差距越大, 则代价函数的值就越大.  
> y 即选择的类别, 0 or 1.  

简化:  
因为 y(类别) 的值总是 0 或 1, 那么就可以将分段函数简化为一个.  

$Cost(h_θ(x), y) = -y log(h_θ(x)) - (1-y)log(1-h_θ(x))$  


#### 梯度下降

同样通过梯度下降的方式来迭代参数 θ.  

$$ θ_j = θ_j - α \frac{∂}{∂θ_j}J(θ) $$
$$ θ_j = θ_j - α \sum^{m}_{i=1}(h_θ - y)x_j $$

    α 为 学习率, 常量系数.  


#### 其他迭代方法

- 共轭梯度
- BFGS
- L-BFGS

这些算法的实现非常复杂, 效率也高于梯度下降.  
一般直接调用已经写好的库.  


### 多元分类

在多元分类中, 将其转换为多个二元分类问题.  

如: 有三类 A, B, C.  
转为 A类, 非A类; B类, 非B类; C类, 非C类.  
一个 n 元分类可转换成 n 个二元分类处理.  


## 过拟合

当参数量过多, 而没有足够的样本来进行约束, 就容易过拟合.  
欠拟合: 模型不合适, 无法与样本数据较好地拟合.  

解决:  
- 减少不必要的特征变量, 减少参数量.  
- 正则化.  


### 正则化

对代价函数进行改造, 加入 "惩罚项", 限制参数.  

- 惩罚项(正则化项):  
    $λ \sum^{n}_{i=1}θ_i$

$J(θ) = Cost() + 1000 θ_1 + 1000 θ_2 + ...$  

    这样会让 θ 尽可能小且接近, 会让函数更加 "平滑".  
    λ 的值需要与 cost 的值相适应.  

- 迭代 θ:    
    以线性回归为例.  
    梯度下降:  
    $θ_j = θ_j - α [\frac{1}{m} \sum_{i=1}^{m} (h_θ(x) - y)x_j + \frac{λ}{m} θ_j]$  
    $θ_j = θ_j(1 - α \frac{λ}{m}) - α \frac{1}{m} \sum_{i=1}^{m} (h_θ(x) - y)x_j$  
    1 - α λ/m 的值一般略小于 1, 正则化后相当于先将 θ 缩小一点再进行原梯度下降.  

    正规方程:  
    $θ = (X^T X)^{-1} X^T y$  -> $θ = (X^T X + λE)^{-1} X^T y$  
    E 为 单位矩阵.  

    一般对第一个参数 θ_0, 不加入惩罚项.  
    单位矩阵则第一个值取 0.  


## 常用的模型

### 神经网络
[深度学习 - 从理论到实践](深度学习%20-%20从理论到实践.md)  

- 非线性假设:  
    在 **线性回归** 和 **逻辑回归** 中, 需要先设计一个合理的假设模型, 如一次函数, 二次函数... 随着特征量的增加, 参数量也不断增加, 这样一个 m元n次函数将会有非常多的多项式.  
    因此在非线性假设(非线性函数)中, 一般选择使用神经网络.  
    将复杂的非线性方程分解为数个简单的线性方程.  
    <br>

- 神经元:  
    大脑中的神经元可以大致分为三部分, 树突, 轴突, 细胞核.  
    - 树突, 数量较多, 用来接收其他神经元的信号.  
    - 轴突, 单个, 传递输出信号给其他神经元.  
    - 细胞体, 可以认为是计算中枢.  
    <br>

- 神经网络结构:  
    - 输入层(input layer): 接收输入数据.  
    - 隐藏层(hidden layer): 处于中间位置, 可以有多层, 每层的神经元数目一般相等.  
    - 输出层(output layer): 输出结果.  
    - 激活函数: 非线性假设函数g(z), 增加非线性因素, 否则神经网络只有线性组合(n元m次方程), 无法处理分类问题.  
    
    单个神经元表示一个参数向量, 每层的多个神经元组成一个参数矩阵.  
	<br>
- 偏置单元:  
	
    每个神经单元都有一个偏置单元, 其为神经元引入一个偏置项(常数), 在神经元参数与输入值相乘求和后, 加上偏置项.  
    例: $y = activation(w1 * x1 + w2 * x2 + w3 * x3 + b)$


#### 多元分类

神经网络的多元分类模型中常用 y 向量表示结果, 一个 n 维向量, n 即类别个数.  
[0, 0, 1, 0], 1 的位置表示结果为对应类别, 其他应全部为 0.  


#### 反向传播算法

前向传播通过训练数据和权重参数计算输出结果;  
反向传播通过导数链式法则计算损失函数对各参数的梯度, 即各参数对误差的影响, 并根据梯度进行参数的更新.  

- 反向传播算法步骤:
    1. 正向传播, 通过数据和参数计算输出结果 y 以及记录所有中间变量.  
    2. 计算输出层误差(通过代价函数/损失函数), 得到误差/输出层梯度 δ.  
    3. 从输出层开始反向传播, 计算各层各节点的梯度, 并进行参数的更新.  
    4. 重复以此上步骤.  
	<br>

- 计算误差(通过 代价/损失函数):  
    $δ = \frac{∂Layer}{∂a} = a - y$.  
    即 误差 等于输出结果减标准结果.  
    输出层误差 也称为 输出层梯度.  
    <br>
    
- 通过误差计算隐藏层每个参数的梯度:  
    隐藏层第n层的梯度矩阵: $δ^n =  (Θ^{n+1})^T δ^{n+1} · g'(z^n)$  
    > $Θ^{n+1}$ : 第 n+1 层的参数θ组成的矩阵, 不包含偏置项.  
    > $δ^{n+1}$ : 第 n+1 层的误差梯度矩阵.  
    > $g'(z^n)$ : 第 n 层激活函数的导数, 为可计算出的常量值.  

- 参数更新:  
    非偏置项的梯度 : $Δ^{l+1} = \frac{∂J(Θ)}{∂Θ} = a^l δ^{l+1}$  
    更新: $Θ^l -= α Δ^l$  
    > $a^L$ : 第 L 层的输出.  
    > α : 学习率.  


#### 梯度检测

在反向传播过程中, 可以通过梯度检测来验证其计算是否出错.  
即在反向传播求出偏导后, 通过正向传播求偏导, 验证误差. 如果两种方法计算出的梯度相近, 则表示算法正确.  

$$\frac{∂f}{∂x} = \frac{f(x + \Delta x) - f(x - \Delta x)}{2\Delta x}$$  
> 其计算速度较慢, 在训练过程中不使用, 只在验证时使用.  


#### 随机初始化

如果将神经网络中的参数都初始化为 0, 会导致同神经元的梯度相等, 即其参数会一直相等.  
所以在参数初始化时会将其设置为 \[-x, x] 间的随机值, 其值接近 0.  


#### 模型诊断

当完成训练后, 模型的评估或许与实际有较大出入, 需要对模型的准确度进行评估.  

- 数据集分类:  
    一般会将一部分数据从数据集中分离出来(随机), 用来进行测试. 剩下的用来训练.  
    1. 训练集 : 测试集 = 7 : 3  
        训练完成后, 用测试集判断模型的拟合情况.  
	<br>
    2. 训练集 : (交叉)验证集 : 测试集 = 6:2:2  
        验证集: 用于筛选模型, 先通过训练得到不同假设函数(超参数)的各种模型, 用验证集选出最合适的模型, 最后用测试集判断训练的拟合程度.  
    
    误差判断都使用代价函数 J(θ).  
    <br>

- 可能出现的问题:  
    
    - 高偏差:  
        对训练集的拟合程度较差, 对测试集/验证集的拟合较差. 出现欠拟合现象.  
        **可能的原因**:  
        二项式次数过低, 正则化项的系数(λ)过大.  
        特征数量少, 不能很好地通过特征拟合结果.  
	<br>

    - 高方差:  
        模型对训练数据的拟合度高, 对测试集/验证集的拟合度低. 出现过拟合.  
        **可能的原因**:  
        二项式系数过高, 正则化项系数(λ)过小.  
        特征选取过多.  
        训练数据较少.  

随着多项式的增加(或λ减小), 由 **欠拟合(高偏差)** 到 **适合** 到 **过拟合(高方差)**.  

![误差变化](https://pic.imgdb.cn/item/64bcc47d1ddac507ccbe3048.jpg)

- 解决方法:  

    按问题原因使用对应的方法即可:  
    - 增加训练样例.  
    - 增加/减少选取的特征.  
    - 增加/减小正则化项系数 λ.  


##### 不对称性分类的误差评估

- 不对称性分类:  

    指分类中有一类的比例远高于另一类(99:1).  
    此时, 模型的分类结果的准确度(95:5)可能还不如直接判定全部为比例高的一类(100:0).  
    但全部判为一类显然不是一个好的模型.  


- 结果分类:  

| 实际分类\\预测分类 |   0    |   1    |
|:------------------:|:------:|:------:|
|         0          | 真阴性 | 假阳性 |
|         1          | 假阴性 | 真阳性 |

True positive, False negative.  

- 查准率(precision)和召回率(recall):  

    $precision = \frac{真阳性}{真阳性 + 假阳性}$  
    $recall = \frac{真阳性}{真阳性 + 假阴性}$  


### 支持向量机

支持向量机, Support vector machine, SVM.  
为一种大间距二分类模型.  

代价函数:  
$$
\min_{\theta} C \sum^m_{i=1} y^i cost_1(\theta^T x^i) + (1 - y^i) cost_0(\theta^T x^i) + \frac{1}{2} \sum^n_{i=1} \theta^2_j
$$
> $cost_0, cost_1$:  
> 在逻辑回归中, 会将假设函数带入代价函数中以求梯度.  
> $-y log\frac{1}{1 + e^{-z}} - (1 - y)log(1 - \frac{1}{1 + e^{-z}})$  
> log与e计算量较大, 所以用近似的线性(一次)函数代替.  

$$ -log\frac{1}{1 + e^{-z}} ≈ \left\{
	\begin{aligned}
	  \frac{3}{4}-\frac{3}{4} z \quad if && z ≤ 1 \\
	    0 \qquad if && x > 1 \\
	\end{aligned}
	\right.
$$

> $-log(1 - \frac{1}{1 + e^{-z}})$ 同理, 与其对称.  
>
> C :  
> 原本的正则化项会加 λ 系数, 这里将其放置前一项中, 用 C 代替.  
> 可视为 C = 1 / λ, 不影响最小化后的 θ值.  
> $\frac{1}{2} \sum^n_{i=1} \theta^2_j$ 可替换为 $θ^T θ$.   

- 大间距分类器:  

    当有多个决策边界可以实现分类时, 向量机会选择分类间隔的中间位置作为边界, 即向量机是一个大间距分离器.  
    **原理**:  
	    此时代价函数第一项($\sum^m_{i=1} y^i cost_1(\theta^T x^i) + (1 - y^i) cost_0(\theta^T x^i)$) 等于或约等于 0.  
    决策边界线: $\Theta^Tx ≈ 0$, 即θ向量近似垂直于边界线.  
    只需要最小化 $\sum^n_{i=1} \theta^2_j = ||θ||^2(θ向量长度的平方)$.  
    如果边界线靠近样本点, x向量在θ向量的投影会更短, 为保证第一项约为 0, θ向量的长度会更大.  
    所以在最小化 θ 时, 边界线会尽可能远离两边样本点.  


#### 核函数

- 复杂的非线性分类器:  

    一般非线性分类需要高次的多项式来划分边界.  
    为了简化复杂的高阶多项式, 对特征变量进行改造.  
    θf 取代 θX, X 为初始特征向量, f 为"核函数"构造的新特征向量.  
	<br>

- 核函数(kernel):  

    $f^i = k(x, l^i)$  
    为构造新的特征向量使用核函数(Gaussian kernel, 高斯核函数).  
    $f^i =  similarity(x, l^i) = exp(- \frac{||x - l^i||^2}{2σ^2})$  
    > $f^i$ : f 向量的第 i 个特征变量.  
    > l: 选取的样本的特征向量.  
    > σ: 常量参数, 用于调整 f 的大小.  
    > similarity: 本质为计算数据的特征向量与各样本向量的"欧式距离", 以此为新特征.  
    > exp(x): 表示 e^x.  
    因此特征向量(f)的维度 n, 等于样本(l)的数量 m + 1, 多的一个为偏置项.  

    以此可以将非线性模型转化为线性模型.  
    θf ≥ 0, 以此作为分类边界.  
	<br>

- 特征缩放:  

    在计算特征向量之间的距离时, 特征之间的数值差距可能会很大, 造成一些特征由于数值小, 其影响直接被忽略, 所以需要将特征的值进行缩放, 使各特征的数值相对平衡.  
    <br>

- 其他核函数:  

    略.  


相比于神经网络, SVM可能拥有更快的优化速度, 可以大幅度减少训练时间.  



### 无监督学习

无监督学习, 样本并不带有目标值(分类), 通过算法自动将样本集归类分成**子集**或**簇**.  
这些算法称为聚类算法.  


#### K-means算法

K-means, 即K均值.  

二分类时:  
1. 设定两个 **聚类中心**(分类簇的中心, 初始值随机).  
2. 计算各样本到两个中心的距离, 并以此分成两类.  
3. 计算两类的中心点, 将聚类中心移至中心点.  
4. 重新计算到中心的距离, 并再次分类.  
5. 以此重复, 直到分类结果不变.  

K个分类时只需初始化K个聚类中心, 其他步骤相同, K的具体数值需要按**自己的需求**或**样本的分布**来**人为决定**, 其值也不唯一.  


- 初始化:  
    建议随机选择 K个样本作为初始聚类中心.  
    分类可能会落在局部最优解上, 而非全局最优解. 可以尝试多次初始化, 找到全局最优解.  
    <br>

- 代价函数:  
    其代价(损失)函数即样本到聚类中心的距离.  
    先最小化单个样本到不同聚类中心的代价, 即进行分类.  
    再最小化同类样本到聚类中心的代价, 即移动聚类中心.  
	<br>

### 数据压缩(主成分确定)

当数据的多个特征高度相关时, 可以对其进行降维, 将多个特征合并为一个特征.  

#### PCA算法

主成分分析算法. 将高维特征向量降为新的低维特征向量, 减少特征维度.  
其与线性回归相似但不同. 
- 线性回归的代价为预测与实际间的方差; PCA的代价为点到线(面)的距离.  

直角坐标系中的向量可用一组向量基表示(一般选择正交基), 如果基的数量小于特征向量的维度, 即可实现降维效果.  
降维即求解这组向量基.  

步骤:  
1. 数据处理: 均值标准化, 数据缩放.  
	- 均值标准化: 求均值, 实际数据 - 均值. 以此表示数据, 方便计算.  

2. 计算协方差(矩阵):  
	$Cov(x^i, (x^i)^T) = \Sigma = \frac{1}{m} \sum^n_{i=1} (x^i)(x^i)^T = \frac{1}{m} XX^T$  
	- 协方差: 表示两个变量的相关性, 用Σ表示, 与求和符号重复了, 注意区分.  
	- x^i为特征向量(nx1),Σ为 nxn 矩阵. m 为样本个数.  
	- X 为 nxm 样本特征矩阵.  
	- Σ主对角为变量方差, 其他为变量协方差(需要优化为0).  

3. 奇异值分解(SVD):  
	[U, S, V] = svd(Σ)  
	- U 为 nxn 矩阵, 每列为一个向量基, 取前k个即可.
	- $U_{reduce} = U_k$, 即取前k列的矩阵.  

4. 降维:  
	$Z = U_{reduce}^T X$
	- U^T 为 kxn 矩阵, X 为 nxm 矩阵.  


#### 压缩重现

指将压缩后的数据重新映射回高维.  
根据上面的降维公式:  
$Z = U_{reduce}^T X$  
则, $X = U_{reduce} Z$  

其只是计算压缩数据在高维的位置,即压缩为**有损压缩**.  


#### k值选择

以数据的方差表示表示数据的离散程度, 从而代表数据"信息含量".  

均方投影误差(v): $\frac{1}{m} \sum^m_{i=1} ||x^i - x^i_{approx}||^2$  
- x_approx: 为样本特征投影后在高维向量中的坐标.  
数据总方差(v_0): $\frac{1}{m} \sum^m_{i=1} ||x^i||^2$  

$V = \frac{v}{v_0} ≤ x$ 
表示(1-x)的方差被保留下来.x 一般取小于 0.1, 即保留 90% 以上的方差.  

在奇异值分解中得到:  
[U, S, V] = svd(Σ)  
S 主对角线的值对应特征的方差比例.  
如: 取 k 为 3, $v = S_{11} + S_{22} + S_{33}$  
$V = 1 - \frac{\sum^k_{i=1} S_{ii}}{\sum^n_{i=1} S_{ii}} ≤ 0.01$  


#### PCA的使用

$U_{reduce}$ 的确定应只用训练集, 确定后可应用于交叉验证集和测试集.  
即 $U_{reduce}$ 的值只在训练时可改变, 在测试和应用中不再更改.  

虽然 PCA 可以减少数据维度(特征数量), 但其保留了数据大部分的方差, 特征信息并没有丢失多少.  
所以使用 PCA 防止过拟合并不是一个好的选择, 请使用正则化.  


### 异常检测

同样本的特征数据集中在一定范围内, 可以用样本数据判断新数据是否"异常".  

- 请先复习**高斯分布(正态分布)** 和 **最大似然估计**.  

1. 选择特征, 获得样本数据.  
2. 拟合各个特征的正态分布概率密度.  
$$\mu_{j} = \frac{1}{m}\sum^m_{i=1} x_{j}^i$$
$$\sigma_j = \frac{1}{m} \sum^m_{i=1} (x^i_{j} - \mu_{j})^2$$
3. x的概率为:  
$$p(x) = \prod^n_{j=1} p(x_j;\mu_{j},\sigma_{j}^2) = \prod^n_{j=1} \frac{1}{\sqrt{ 2\pi }\sigma_{j}} \exp\left( -\frac{(x_{j} - \mu_{j})^2}{2\sigma^2_{j}} \right)$$
4. 可以根据选择的概率界限ε来划定一个范围, 得到一条分界线.  


- 与监督学习的比较:  

	- 当大多数样本为同一类, 另一类出现的概率非常小时, 可以使用异常检测, 正负样本的比例没有太大偏差时, 可以使用监督学习进行分类.  
	- 正常样本的特征较为稳定, 而异常样本的特征不确定, 此时可以用异常检测, 而不是建立分类模型.  


- 参数调整

	并非所有数据都服从正态分布, 对应不服从的数据, 可以进行调整(变换), 使其服从正态分布.  
	
	$x \to log(x)$  
	$x \to x^\frac{1}{2}$  
	...


- 多元正态分布:  

	之前的概率模型要求变量之间相互独立, 对于不独立的变量之间的异常预测不准.  
	对此无法使用数据压缩进行降维, 对正常数据其特征之间相关, 但异常数据特征脱离了其特征的相关性, 会被压缩进正常范围内.  

	多元正态分布概率公式:  
	$\mu = \frac{1}{m} \sum^m_{i=1}x^i$  
	$\Sigma = \frac{1}{m} \sum^m_{i=1} (x^i - \mu)(x^i - \mu)^T$  
	$p(x; \mu, \Sigma) = \frac{1}{(2\pi)^{n/2} \mid \Sigma\mid^\frac{1}{2}} \exp\left( -\frac{1}{2}(x-\mu)^T\Sigma^(-1)(x-\mu) \right)$  
		$\Sigma$ : 协方差矩阵(nxn), 以此代替方差,. 
		注意, 此时的 $\mu$ 也是一个向量.   
		当然, 这个公式并不需要去记住, 会调库就行().  

原概率模型也可以通过用已有特征构建新特征代替来消除相关性(如,用 x1/x2 代替 x1, x2), 原模型的计算简单, 能适应大量特征, 而且使用矩阵时要求 m > n, 否则协方差矩阵的倒置将不可逆.  
所以一般还是不用多元正态分布, 当 n 较少, 且 m > n 时才考虑使用.  


## 应用

### 基于内容的推荐算法

#### 特征

将推荐的对象(电影, 商品)按内容等设置一组特征 X, 每个用户可以训练出一组变量 θ.  

$θ^TX$ 计算出的值即代表此用户对某个对象的喜爱程度.  

其训练方式与线性回归一致.  

#### 协同过滤

通常, 想对推荐的对象一个个设置特征向量是不现实的, 可以通一些方法反向推断.  

1. 先让用户选择自己的偏好(比如爱情电影), 即得到用户的 θ 参数.  
2. 如果用户给一部电影打五星, 可以通过此结果反向计算该电影的特征向量.  
3. $X\theta \approx 5$  
4. 通过θ1, ... , θn 得到 x^i:  

得到 x 后也可以用 x 迭代 θ, 再迭代 x, 再迭代 θ...
当然, X、θ也可以同时进行优化.  

$$\min_{x, \theta} \frac{1}{2} \sum_{(i,j):r(i,j)=1} ((\theta^j)^T x^i - y^{(i,j)})^2 + \frac{\lambda}{2} \sum^n_{i=1} \sum^n_{k=1}(x^i_k)^2 + \frac{\lambda}{2} \sum^n_{i=1} \sum^n_{k=1}(\theta^i_k)^2$$


- 向量实现:  

	如果将所有用户对所有对象的评分作为一个矩阵, Y_(i,j)为用户对对象j的评分.  
	$Y = \Theta^T X$   $Y_{i,j} = (\Theta ^i)^T x^j$  
	因为评分矩阵中有大量缺失值, 所以 Y 为一个低秩矩阵. 可以进行低秩分解.  
	这样可以根据已知值填补缺失值.  


- 均值归一化:  

	当一个用户没有对任何对象进行评分时, 根据上面的最小化公式, 其偏好参数会全部为 0, 那么模型对他的评分会全部预测为 0.  

	归一化: 计算对象评分的均值, 将评分用均值的正负分表示.  
	评分计算: $(\theta^i)^T x^i + \mu(均值)$  
	此时即使 $(\theta^i)^T x^i$ 为 0, 也会加上均值, 即评分结果为均值.  


### Photo OCR

Photo OCR, (照片光学字符识别).  

1. 识别文字区域: 将带有文字的区域提取出来.  
2. 字符分割: 将文字区域中的文字分割成单个字符.  
3. 字符识别: 识别单个字符, 可能带有纠错.  


#### 滑动窗口

对于图片中特定内容的识别, 采用一个小窗口不断滑动的形式来判断, 然后窗口增大, 再遍历一次... 但文本的矩形长宽比例不定, 难度较大.  

先识别字符区域, 判断窗口内是否为字符, 将所有的字符区域进行标记, 并进行扩大, 使它们连在一起成为一个类似矩形的区域, 最后可以根据区域的长宽比判断是否为文本区域.  


## 大规模机器学习

当训练样本数据量非常大时, 其在计算代价、进行梯度下降时会非常吃力.  


### 随机梯度下降

因为在计算偏微分求梯度下降方向时, 需要对样本代价求和, 当数据量太大时, 使计算变得非常困难. 此方法称为批量梯度下降.  


- 随机梯度下降:  

	不需要对整个样本集计算梯度, 只需先进行随机排序, 再不断抽取样本, 进行优化更新.  
	随机抽取能更接近整体.  

	这样能有更快的更新速度, 但精确度会下降, 每次更新方向不一定是最优解, 可能会非常"扭曲"; 也难以并行.  

	抽取的样本可以为一个或一组(Mini-batch梯度下降).  


- 收敛:  

	对于批量梯度下降算法, 我们可以通过绘制代价(损失)曲线来判断是否收敛. 但对于随机梯度下降, 个体的代价不能很好地反映整体.  
	所以每隔一定迭代次数(1000)绘制一次代价的平均值, 通过观察这条折线来判断是否收敛. 学习率, 每组的样本量, 绘制的迭代间隔都会影响折线的样子.  
	其最后会不断在收敛值上下波动, 难以趋近收敛, 或者可以通过减小学习率来让它趋近收敛值(学习率衰减, 与迭代次数负相关).  


![随机梯度下降](https://pic.imgdb.cn/item/64bcc8261ddac507ccc92c98.jpg)


### 并行计算

$\theta_{j} := \theta_{j} - \alpha \frac{1}{m}\sum_{i=1}^m (h_{\theta}(x^i) - y^i)x^i_{j}$  
累加求梯度的部分可以分配给多个电脑进行并行计算.  
将训练数据集等分成多份, 并行计算代价的偏导和, 最后在中心服务器上汇总计算.  

只要学习算法可以表示成一系列的求和形式或对训练集的求和形式, 就可以进行并行计算.  

- MapReduce:  
	分布式计算框架, 将数据进行切片, 并行计算, 然后进行汇总计算.  


---


# 常用科学计算库

## matplotlib模块

一个常用的 python 2D 绘图库(元老级的).  
很多其他python绘图库都是基于matplotlib开发的, 如seaborn, ggplot等.  

- matplotlib 三层结构:
    - 容器层: 由 Canvas(画板), Figure(画布), Axes(绘图区/坐标系)组成.  
        Canvas位于最底层, 充当画板, 用来放置Figure.  
        可以用 plt.subplots() 将画布分割成多个绘图区.  
    - 辅助显示层: 
        放置各种标题, 网格, 图例等图像信息.  
    - 图像层:  
        即各种绘制的图像.  

| 方法 | 说明 |
| --- | --- |
| figure(figsize=(), dpi=) | 创建画布, figsize为图的长宽, fig为清晰度 |
| plot(x:list, y:list) | 绘制折线 |
| scatter() | 绘制散点图 |
| bar() | 绘制柱状图 |
| hist(x, bins) | 绘制直方图, bins为组距 |
| pie() | 绘制饼图 |
| show() | 显示图像 |
| xticks(list) | 修改x轴刻度 |
| grid(True, linestyle='--', alpha=0.5) | 添加网格线, '--'表示虚线, alpha为透明度(0-1) |
| xlabel(str) | 添加x轴描述信息 |
| title(str) | 添加标题 |
| savefig(str) | 保存图像, 参数为保存路径, 需要在show()之前 |
| legend(loc="best") | 显示图例, 参数为位置 |


### 折线图

- pyplot模块:  
    其包含一系列类似于matlab的画图函数.  
    作用于当前图形(figure)的当前坐标系(axes).  
    `import  matplotlib.pyplot as plt`  


```python
# 图像中中文字体无法显示, 可修改配置文件或暂时修改配置:
from pylab import mpl
# 设置显示中文字体, 字体需要自行下载并安装, "SimHei"为字体名.  
mpl.rcParams["font.sans-serif"] = ["SimHei"]
# 防止部分字符无法正常显示
mpl.rcParams["axes.unicode_minus"] = False

import matplotlib.pyplot as plt
# 创建画布
plt.figure(figsize=(20, 8), dpi=100)

# 绘制图像
# 获取x,y轴数据
x = range(60)
y_temper = [random.uniform(15, 18) for i in x]

# 添加x,y轴
# 多次plot可获得多条折线
plt.plot(x, y_temper, color='r', label='G市')

# 获取x, y轴刻度
x_ticks_label = ["11点{}分".format(i) for i in x]
y_ticks = range(40)
# 修改x, y轴刻度
# 坐标刻度不可以直接通过字符串修改
# plt.xticks(x_ticks_label[::5])
plt.xticks(x[::5], x_ticks_label[::5])
plt.yticks(y_ticks[::5])

# 网格
plt.grid(linestyle="--", alpha=0.5)

# 添加描述信息
plt.title("某时段的天气温度变化")
plt.xlabel("时间")
plt.ylabel("温度")

# 图例, 需要先在折线添加label属性
plt.legend()

# 图像显示
plt.show()
```

- 多个绘图区:  
    figure, axes = plt.subplots(nrows=1, ncols=1, figuresize...)  
    参数为行数与列数.  
    返回值为画布, 坐标. 画布为共用.  
    用 aese\[row]\[col] 代替 plt 表示画布.  
    axes\[0]\[0].plot()


### 其他图表

- 散点图:  
```
x = [x1, x2, x3...]
y = [y1, y2, y3...]

plt.scatter(x, y)
```

- 柱状图:  
    plt.bar(x, y)  
    当在同一坐标系中在画出柱状图时, 如果 x轴相同, 后者会覆盖前者.  
    通过改变 x轴, 可以控制柱子的位置.  

- 分布直方图:
    plt.hist(data, bins, density=False)  
    - 数据, 分组数, density: 显示为频率.

- 饼图:  
    plt.pie(data, labels, autopct, color)  
    - 数据, 各组的名称, 占比显示指定(%1.2f), 各组颜色  


## Numpy

Numpy是一个开源的Python科学计算库, 用于快速处理任意维度的数组.  
Numpy支持常见的数组和矩阵操作.  
Numpy使用ndarray对象来处理多维数组,该对象是一个快速而灵活的大数据容器.  

- ndarray属性:  
    - shape: 形状, 表示array各维度的长度.  
    - dtype: 数据类型, 其元素的数据类型.  


| 方法                                   | 说明                                                  |
| ------------------------------------ | --------------------------------------------------- |
| numpy.array(list)                    | 返回n维数组, 参数为n维的数组                                    |
| ones/zeros(\[n, m])                  | 生成n行m列的1/0数组                                        |
| ones/zeros_like(array)               | 生成大小与array相同的n维1/0数组                                |
| linspace(start, stop, num, endpoint) | 生成等差数列, num为元素个数(默认50), endpoint表示是否包含stop值(默认ture) |
| arange(start, stop, step, dtype)     | 创建等差数组, step为步长(默认1)                                |
| logspace(start, stop, num)           | 创建等比数列, num为元素个数(默认50)                              |
| random.normal(loc, scale, size)      | 生成size个正态分布的随机数, 均值为loc, 标准差为scale                  |
| random.uniform(low, high, size)      | 生成size个均匀分布的随机数, 范围: low-high                       |
| reshape(shape, order)                | 返回一个具有相同数据域, 但shape不同的数组                            |
| resize(new_shape)                    | 修改数组本身的形状                                           |
| T                                    | 转置符, 调用后将矩阵转置                                       |
| astype(type)                         | 返回修改元素数据类型之后的数组, type为某数据类型                         |
| tostring()                           | 序列化, 转换成bytes                                       |
| unique(array)                        | 将传入的数组去重                                            |
| copy(ndarray)                        | 深拷贝                                                 |

- reshape(shape):  
    shape, 应为一个数组\[x, y, z, ...], 表示各维度的元素个数.  
    如: \[3, 4, 2], -1 表示不确定的个数, \[-1, 5]表示行数不确定, 但只有2列.  
    有-1时, 如果形状不能完全填充就会报错.  

- 切片:  
    多维数组也可以进行切片, narray\[n:m, x:y], 按维度一个一个切, 中间用 "," 隔开.  


### ndarray运算

1. 逻辑运算:  
    ```python
    import numpy as np
    
    # 生成10行5列, 40-100之间的随机整数
    score = np.random.randint(40, 100, (10, 5))
    
    # 切出后四行数据用于测试
    test_score = score[6: , 0:5]
    
    test_score[test_score > 60] = 1
    # test_score > 60, 此表达式会返回其结果的bool数组.  
    # ndarray 可以根据bool索引进行访问(访问其值为True的部分)
    ```

2. 通用判断函数:  
    - all(): 判断元素是否全部符合条件.
    - any(): 判断是否存在符合条件的元素.
    - 参数为bool数组.  

    ```
    # 数据接第一部分
    # 切取前两名学生的成绩
    np.all(score[0:2, :] > 60)
    
    np.any(score[0:2, :] > 90)
    ```

3. np.where(三元运算符):  
    - 参数1: 一个bool数组.  
    - 参数2: 将其中True修改为的值.  
    - 参数3: 将其中False修改为的值.  

    ```
    temp = score[:4, :4]
    
    np.where(temp > 60, 1, 0)
    
    np.where(np.logical_and(temp > 60, temp < 90), 1, 0)
    
    np.where(np.logical_or(temp > 90, temp < 60), 1, 0)
    ```

4. 统计运算

| 运算                   | 说明                    |
| ---------------------- | ----------------------- |
| min(a, axis)           | 返回最小值              |
| max(a, axis)           | 返回最大值              |
| median(a, axis)        | 计算中位数              |
| mean(a, axis, dtype)   | 计算平均值              |
| std(a, axis, dtype)    | 计算标准差              |
| var(a, axis, dtype)    | 计算方差                |
| argmin/argmax(a, axis) | 返回最小/大值对应的下标 |
> a为数组: 或者以 a.min(axis) 的形式调用.  
> axis: int 为行或列, 会计算每行(列)的统计值等. 可以为空, 此时统计范围为全部数据.  
> 另一种调用方式: a.min(axis)


### 数组间的运算

1. 数组与数的运算
    ```
    a = np.array([[1,2,3], [3,4,5]])
    
    a + 3   # 返回全部元素 +3 后的数组
    a / 2   # 返回全部元素 /2 后的数组
    
    a = [1, 2, 3]   # 列表
    
    a * 3   # 列表会复制成3份, 列表与nparray的运算规则大不相同.  
    ```

2. 数组与数组的运算
    同位置的元素进行运算.  

    不同形状数组之间的相加会通过**广播机制**进行扩展, 使两数组形状相同.  
    两数组应满足:  
    - 维度不同, 低维数组前扩展维度, 长度为1, 使其维度相同.  
    - 维度相同, 对应维度长度相同, 或其一为1.  
    例如: (2, 3) 可以看作 (1, 2, 3), (1, 1, 2, 3), (1, 1, 1, 2, 3). 只要相同维度的长度相同或其中一个为 1 即可进行运算.  


### 矩阵与向量

矩阵可以看作二维数组, 向量为 n x 1 的数组.  
矩阵可以用 array() 或 mat() 储存.  

「请」学习《线性代数》.  
[线性代数](../数学/线性代数.md)  

矩阵乘法: np.matmul(a, b), np.dot()  
- matmul中不支持矩阵与标量相乘.  


### 合并与分割

- hstack((array1, array2)):  
    水平拼接: 将同行的元素拼接至同行.  

- vstack((array1, array2)):  
    竖直拼接: 将同列的元素拼接至同列.  

- concatenate((a1, a2), axis=):  
    axis: 0, 按行拼接; 1, 按列拼接.


### I/O与数据处理

详见pandas中.  
[DataFrame缺失值处理](#缺失值处理)


## Pandas

### 简介

- 专门用于数据挖掘的开源python库
- 以Numpy为基础
- 基于matplotlib, 能够简便画图
- 独特的数据结构

### pandas的数据结构

#### series(一维数组)

- 创建: pandas.Series(data, index= , dtype= )
    - data也可以为字典, 此时索引即为对应的key.  
    - index: 索引, 必须唯一, 且与数据长度等长. 如果没有传入则自动创建0-N的整数索引.  
    - dtype: 数据的类型, 可省略.  
- 属性(可直接访问):  
    - index: 索引
    - values: 值
    还可以通过 <series数组名>\[下标] 的方式访问对应的值.  

#### DataFrame(二维数组)

- 创建: pd.DataFrame(data, index= , columns= )
    - index: 行标签, 可以为string列表, 默认0-N.  
    - columns: 列标签, 可以为string列表, 默认0-N.  

- 属性: shape, index, columns, value...    
    - T: 转置
    - head(n): 获取前n行数据, n默认为5.  
    - tail(n): 获取后n行数据, n默认为5.  

- 设置索引: <DataFrame对象名>.index/columns = list.  
    - 重置索引方法: reset_index(drop=True)  drop默认为false.
    - 以某列为索引: set_index(key, drop=True)  drop默认为True.

- 形式:  

|      | columns |
| ---- | ----- |
| index| value |


#### MultiIndex与Panel

MultiIndex: 多级索引(层次化索引), 是pandas的重要功能. Series, Data Frame对象内也有MultiIndex对象.  

- 创建: pd.MultiIndex.from_arrays(arrays, name= )
    - arrays: 索引列表, 二维string数组.
    - name: string数组, 代表每个索引的名字.
- 属性:  
    - names: 索引的名字
    - levels: 索引(有序列表)
    - labels: 索引对应的顺序

拥有三个索引的数组就是三维数组, panel为储存三维数组的容器.  

- 创建: pandas.Panel(data, items, major_axis, minor_axis)
    - 参数为数据与三个索引

无法直接查看, 只能先切成二维数组再查看.  



### 排序

- sort_values(by= , ascending= ) : 根据值排序
    - by: 排序依据的索引(依据哪个索引排序), 可以为多个.  
    - ascending: 默认为True(升序), False为降序.  

- sort_index() : 按索引值排序.  


### DataFrame

- 算术运算  
    1. df\[column].add(...)   
    2. df\[column] + ... (不推荐)   

- 逻辑运算  
    1. df\[column] > ...  
        返回布尔索引, 即一个布尔数组.  

    2. df.query(expr:string)  
        expr为查询字符串, 形式为: "索引值 > 60 & 索引值 < 80 ", 此时此索引下的值大于60且小于80的会返回true.  

    3. df\[column].isin(list)  
        判断此索引下的数据是否在 list 中.

- 统计运算:  
    - describe(): 综合分析, 能够直接得出count, mean, std, min, max等
    - 个单独的统计函数, sum, mean等, 具体可参考Numpy.  

- 自定义运算: apply(fun, axis=0)  
    - fun: 自定义函数
    - axis: 默认(0)为列, 1为行


#### DF筛选

取某行:  
返回筛选后的记录.  

- 单条件筛选:  
    根据某列属性筛选出对应记录:  
    df\[df.column_name == 'value']  
    df\[df.index == 10]

    - contains 函数:  
        df\[df.column_name.str.contains(str)]
        筛选出某筛选中含有 str 的记录.  

    即以 df\[<bool索引>] 的方式筛选.  

- 多条件筛选:  

    - 对多个属性(字段)的筛选:  
        df\[(<bool索引>) | (<bool索引>) & (<bool索引>)]  
        "&" 即 and, "|" 即 or, 但不能使用 "and", "or" 代替.  

        df.query("column_name == value & ... | ... ").

    - 单一属性的多个值:  
        df\[df.column_name.isin(<list_name>)]  
        类似 python 中的 "in".  


取某列:  
返回全部记录的某些列.  

df\[column_name]

df.loc\[:, column_name] / df.iloc\[:, column_index]


#### loc 与 iloc

1. df\[column]\[index] 或 df\[column] 获取对应元素.  

2. df.loc\[index, column]  
    根据行和列的索引定位数据, 可以是: 索引名, 切片格式(1: 10), 列表, bool索引.  
    当无column时, 其可直接去掉, 切出指定行; 当无 index 时, 其应用切片代替, loc\[:, column].  

3. df.iloc\[index, column]  
    index 与 column 的格式可以为:  
    整数下标, 列表, 切片格式, bool索引, 不能使用索引名.  


#### to_dict

将 df 转为 python 字典.  

df.to_dict(orient, into)  
- orient:  
    可选: dict(默认), list, series, split, records, index.  
    
| orient= | 转换的格式                                                                          |
| ------- | ----------------------------------------------------------------------------------- |
| dict    | {column1: {index1: value, index2: value, ...}, column2...}}                         |
| list    | {column1: \[values], column2: \[values], ...}                                       |
| series  | {column1: Series(values), column2: Series, ...}                                     |
| split   | {index: \[indexs], columns: \[columns], data: \[\[记录1], \[记录2], ...]}              |
| records | \[{column1: value, column2: value, ...}, {column1: value, column2: value, ...}, ...] |
| index   | {index1: {record1}, index2: {record2}, ...}                                         |


### Pandas画图

Series/DataFrame.plot(kind), Series 或 DataFrame 对象可直接调用plot进行绘制图像, 参数为图像的种类, 默认为"line(折线图)".  

### 文件读取与存储

pandas支持多种文件格式的读取, 包括 CSV, Json, HTML, Lacol, HDF等.  

#### CSV(.csv)

- pandas.read_csv(filepath_or_buffer, sep=',')
    - filepath_or_buffer: 文件路径.  
    - usecols: 指定读取的列名, 列表形式.
- DataFrame.to_csv(path_or_buf, sep=',', columns=None, header=True, index=True, mode='w')  
    - sep: 分隔符,默认为','.
    - columns: 需要写入的列索引.
    - header: 是否写入列索引值, 默认True.
    - index: 是否写入行索引值.
    - mode: 'w'为写入, 'a'为追加.

#### HDF5(.h5)

- pandas.read_hdf(path_or_buf, key=None) 
- DataFrame.to_hdf(path_or_buf, key)

HDF5文件的读取和存储需要指定一个键, 值为要存储的DataFrame.  
优先选择使用HDF5文件存储.  
1. HDF5在存储时支持压缩, 使用的方式为blosc, 这个是速度最快的也是pandas默>支持的.  
2. HDF5支持跨平台, 可以轻松迁移到Hadoop上面.  

#### Json(.json)

- read_json(path_or_buf, orient, typ='frame', lines=False)
    - 将JSON格式转化成默认的Pandas DataFrame格式.  
    - orient: 数据的显示形式  
        - split: 行索引, 列索引, 数据分开.
        - records: 以 columns : values 的形式输出.  
        - index: 以 index : {columns : values} 的形式输出.  
        - columns: 以 columns : {index : values} 的形式输出.  
        - values: 直接输出值
    - typ: 转换成的数据为dataframe或series.  
    - lines: 按照行读取.  

- to_json(path, orient, lines)

### 高级处理

#### 缺失值处理
[I/O与数据处理](### I/O与数据处理)  

1. 缺失值标记为NaN

    - 判断  
        - pd.isnull(df)  
        - pd.notnull(df)
        - 一般配合np.all()与np.any()使用.  
    - 处理  
        - 删除: dropna(axis='rows')  
            - 不会修改原数据, 需要接收返回值.  
        - 替换: fillna(value, inplace=True)  
            - values: 替换值.  
            - inplace: 是否修改原数据, false则返回新数据.  

2. 不是NaN, 例如"?"  
   
    - 先替换为np.nan, 然后继续处理.  
    - df.replace(to_replace='?', value=np.nan)

#### 数据离散化

- 目的：  
	连续属性离散化的目的是为了简化数据结构, 数据离散化技术可以用来减少给定连续属性值的个数。离散化方法经常作为数据挖掘的工具。  

- 具体内容：  
	连续属性的离散化就是在连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数值代表落在每个子区间中的属性值。  
	例如：有一组身高数据(连续属性)，将身高划分为几个区间：150~165、165~180，180~195，并标记为矮、中、高三个类别，最终要处理成一个"哑变量"矩阵。  
	也就是用矮、中、高代替身高数据。  

- 方法:  
    - pd.qcut(data, q)  # 将数据自动分成数量差不多的q组  
    - series.value_counts()    # 统计分组数据数量  
    - pd.cut(data, bins)    # bins为自己指定的分组区间, 例:\[0,5,10]  

将分组可以转化为表格: 横轴为各区间, 纵轴为数据下标, 值为1表示在此区间.  
- pd.get_dummies(p_count, prefix:string)   # p_count为分好的组, prefix为区间的前缀取名.  

#### 合并

- pd.concat(\[data1, data2], axis=1)  
    将两表按照行或列进行合并, axis=0(默认)为列索引, 1为行索引.  

- pd.merge(left, right, how="inner", on=None)  
    - 可以指定按照两组数据的共同键值对合并或者左右各自  
    - left: DataFrame  
    - right: 另一个DataFrame  
    - on: 指定的共同键
    - how: 按照什么方式连接, 默认内连接  

| Merge method | SQL Join Name |
| --- | --- |
| left | 左连接 |
| right | 右连接 |
| outer | 外连接 |
| inner | 内连接 |

#### 交叉表与透视表

交叉表：用于计算一列数据对于另一列数据的分组个数(用于统计分组频率的特殊透视表), 通常还会借助绘图直观地显示.  
例: 一列"性别(男女)"对另一列"分数及格"之间的统计关系. 使用前应先处理分数数据, 将其转换为"及格(1)"或"不及格(0)".  
- pd.crosstab(value1, value2)

透视表: 将原有的DataFrame的列分别作为行索引和列索引, 然后对指定的列应用聚集函数.  
- data.pivot_table()  
- DataFrame.pivot_table(\[], index)  # 前参数为被统计的列表, 后者为统计的列作为索引.  

#### 分组与聚合

将数据分组并统计个数.  

分组: `dataframe.groupby(key, as_index=False)`  
- key: 分组的列数据, 可以多个.  
- as_index: 是否将分组的列作为索引, 默认True.  

```
col = pd.DtaFrame({'color':['white', 'red', 'green', 'red', 'green'], 'object':['pen', ...], ...})

| index | color | object | price |
|   0   | white | pen    | 5.56  |
|   1   | red   | pencil | 4.20  |
|   2   | green | pencil | 3.20  |
|   3   | red   | ashtray| 0.56  |
|   4   | green | pen    | 2.75  |

col.groupby(['color'])['price'].mean()
col['price'].groupby(col['color']).mean()

```

## 电影数据案例分析

### 需求

一组最近10年1000部最流行的电影数据.  
数据来源: https://www.kaggle.com/damianpanek/sunday-eda/data  

问题1: 我们想知道这些电影数据中评分的平均分, 导演的人数等信息, 该如何获取.  
问题2: 对于这一组电影数据, 如果我们想rating, runtime的分布情况, 应该如何呈现数据.  
问题3: 对于这一组电影数据, 如果我们希望统计电影分类(genre)的情况, 应该如何处理数据.  

### 实现

导入包, 获取数据:  
```
%matplotlib inline  # 解决有些平台matplotlib无法显示
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

# 文件路径
path = "./data/IMDB-Movie-Data.csv"

# 读取文件
df = pd.read_csv(path)
```

1. 问题1:  
- 评分的平均分: `df["Rating"].mean()`  
- 导演的人数: 求出唯一值, 再根据其形状得出.  
    `np.unique(df["Director"]).shape[0]     # df["Director"].unique().shape[0]`  

2. 问题2:  
- Rating进行分布展示  
```
# 创建画布
plt.figure(figsize=(20, 8), dpi=80)
# 绘制图像
plt.hist(movie["Rating"].values, bins=20)

# 添加刻度
max_ = move["Rating"].max()
min_ = move["Rating"].min()

t1 = np.linspace(min_, max_, num=21)

plt.xticks(t1)

# 添加网格
plt.grid()

# 显示
plt.show()
```

- Runtime (Minutes)分布, 与Rating相似.  
```
# 创建画布
plt.figure(figsize=(20, 8), dpi=80)
# 绘制图像
plt.hist(movie["Runtime (Minutes)"].values, bins=20)

# 添加刻度
max_ = move["Runtime (Minutes)"].max()
min_ = move["Runtime (Minutes)"].min()

t1 = np.linspace(min_, max_, num=21)

plt.xticks(t1)

# 添加网格
plt.grid()

# 显示
plt.show()
```

3. 问题3:  
思路: 制作一张统计表, 一行为一部电影, 列为电影的类别, 对应的行列标记为 1. 最后对每一列求和. 或者统计时直接加上.  

```python
# 获取所有的电影类别
# 一部电影可能有多个类别, 其用","隔开
temp_list = [i.split(",") for i in movie["Genre"]]

# 此时temp_list内的元素为list, 每个list代表一部电影的类别
# 继续分割, 将所有类别放在一个list中, 并去重
genre_list = np.unique([i for j in temp_list for i in j])

# 获取 电影个数 x 类别个数 大小的0数组
zeros = np.zeros([movie.shape[0], genre_list.shape[0]])

temp_movie = pd.DataFrame(zeros, columns=genre_list)

for i in range(1000)
    temp_movie.ix[i, temp_list[i]] = 1

genre = temp_movie.sum().sort_values(ascending=False)

# 绘图
genre.plot(kind="bar", figsize=(20, 8), fontsize=16, colormap="cool")
```

## seaborn绘图

Matplotlib虽然已经是比较优秀的绘图库了, 但是它有个令人头疼的问题, 那就是API使用过于复杂, 它里面有上千个函数和参数, 属于典型的那种可以用它做任何事, 却无从下手.  
Seaborn基于Matplotlib核心库进行了更高级的API封装, 可以轻松地画出更漂亮的图形, 而Seaborn的漂亮主要体现在配色更加舒服, 以及图形元素的样式更加细腻.  

导入包(先安装): `import seaborn as sns`  

### 绘制单变量分布

- seaborn.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None, color=None)  
    - a:    表示要观察的数据, 可以是Series, 一维数组或列表.  
    - bins: 用于控制条形的数量.
    - hist: 表示是否绘制(标注)直方图.
    - kde:  表示是否绘制高斯核密度估计曲线.
    - rug:  表示是否在支持的轴方向上绘制rugplot.

### 绘制双变量分布

jointplot为双变量分布图表接口. 中心为散点图, 上侧与右侧为量变量的直方图.  

- jointplot(x, y, data=None, kind="scatter", stat_func=None, color=None, ratio=5, space=0.2, dropna=True)  
    - kind:     表示绘制图形的类型.
    - stat_func:用于计算有关关系的统计量并标注图.
    - color:    表示绘图元素的颜色.
    - size:     用于设置图的大小(正方形).
    - ratio:    表示中心图与侧边图的比例. 该参数与中心图的占比成正比.
    - space:    用于设置中心图与侧边图的间隔大小.

二维直方图:  
通过颜色深浅表示其区域内数据的计数, 适用于较大的数据集.  
sns.jointplot(x="x", y="y", data, kind="hex")  

核密度估计图形:  
sns.jointplot(x, y, data, kind="kde")

绘制成对的双变量分布:  
想要在数据集中绘制多个成对的双变量分布, 则可以使用pairplot()实现, 该函数会创建一个坐标轴矩阵, 并且显示Dataframe对象每对变量的关系. 另外, pairplot()函数也可以绘制每个变量在对角轴上的单变量分布.  

```
# iris(鸢尾花), 这是sns自带的一个数据集, 载入用于测试
dataset = sns.load_dataset("iris")
# 绘图
sns.pairplot(dataset)
```

### 用分类数据绘图

#### 类别散点图

- sns.stripplot(x, y, hue, data, order, hue_order, jitter=False)
    - x, y, hue: 用于绘制长格式数据的输入.
    - data:      用于绘制的数据集, 如果x, y不存在, 则它将作为宽格式, 否则作为长格式.
    - jitter:    表示抖动的程度(仅沿类别轴). 当很多数据点重叠时, 可以指定抖动的数量或者设为True使用默认值.

使用swarmplot(), 数据将不会重叠.  

```python
tips = sns.load_dataset("tips")

sns.stripplot(x="day", y="total_bill", data=tips)
# sns.stripplot(x="day", y="total_bill", data=tips, jitter=True)
```

具体的演示: https://www.bilibili.com/video/BV1pf4y1y7kw?p=51  

#### 类别内的数据分布

- 箱形图  
    箱形图(Box-plot), 又称为盒须图、盒式图或箱线图, 是一种用作显示一组数据分散情况资料的统计图. 因形状如箱子而得名.  
    它能显示出一组数据的最大值, 最小值, 中位数, 及上下四分位数.  

- seaborn.boxplot(x, y, hue, data, orient, color, saturation=0.75, width=0.8)  
    - palette:  用于设置不同级别色相的颜色("r","g","b","y").  
    - saturation:  用于设置数据显示的颜色饱和度.  


- 小提琴图  
    用于显示数据分布及其概率密度.  
    这种图表结合了箱形图和密度图的特征, 主要用来显示数据的分布形状.  

seaborn.violinplot(x, y, hue, data)

#### 类别内的统计估计

- 条形图:  
	sns.barplot(x, y, data)  

- 点图:  
	sns.pointplot(x, y, data)  

### 相关性基本分析

通过**热力图**分析两列数据之间的相关性。

`dataframe.corr()    # 返回容器中每两列数据之间的相关性表格.`  

将相关性表格数据绘制成热力图, 通过颜色深浅直观地显示相关性大小.  

- sns.heatmap(coor_name, square=False, linewidth, annot=False)
> corr_name:  获取的相关性数据变量名.  
> square:     是否为方形.  
> linewidth:  方格之间的空隙大小, 默认0.  
> annot:      显示具体的数值.  

调整图的大小: `plt.figure(figsize=(20, 8), dpi=100)`


### 综合案例-北京租房数据




### 鸢尾花数据集

iris(鸢尾花)数据集时常用的分类实验数据集, 由Fisher1936年收集整理, 时一类多重变量分析的数据集.  

- 特征值4个: 花瓣、花萼的长度、宽度  
- 目标值3个: 三种鸢尾花种类(山鸢尾, 虹膜锦葵, 变色鸢尾)  

#### API

sklearn.datasets

- datasets.load_\*(): 获取小规模数据集, 数据包含在datasets里.  
- datasets.fetch_\*(data_home): 获取大规模数据集, 需要从网络上下载. 参数为目录(默认为 ~/scikit_learn_data/)

获取鸢尾花数据集:  
`iris = sklearn.datasets.load_iris()`

返回的数据类型为 datasets.base.Bunch(字典格式)  
> data: 特征数据数组, 二维numpy.ndarray数组.  
> target: 标签(目标)数组, 一维numpy.ndarray数组.  
> DESCR: 数据描述.  
> feature_names: 特征名, 新闻数据、手写数字、回归数据集没有.  
> target_names: 标签(目标)名.  

# 算法

## Scikit-Learn

Scikit-learn为python的机器学习工具, 包含许多知名的机器学习算法的实现.  
Scikit-learn文档完善, 容易上手, 有丰富的API. 稳定的版本: 0.19.1  

### 安装

`pip install scikit-learn==0.19.1`  
安装需要Numpy, Scipy等库.  
`import sklearn`  


## K-近邻算法

K Nearest Neighbor(KNN算法). 一种分类算法.  
如果一个样本在特征空间中的**k个最相似(即特征空间中最邻近)的样本中大多数属于某一个类别**. 则该样本也属于这个类别.  

距离公式: 根据数据与勾股定理计算距离.  

将一样本分类时, 选择离它 "最近的" k个样本, 它属于k个样本中大多数的那个类别.

### K-邻近算法API

- sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)  
	- n_neighbor: 查询使用的邻居数, 默认为5.  


```python
from sklearn.neighbors import KNeighborsClassifier

# 1.构造数据
x = [[1], [2], [10], [20]]
y = [0, 0, 1, 1]

# 2.训练模型
# 2.1 实例化一个估计器对象
estimator = KNeighborsClassifier(n_neighbors=1)

# 2.2 调用fit方法, 进行训练
estimator.fit(x, y)

# 3.数据预测
ret = estimator.predict([[0]])
print(ret)
```

### 距离度量

- 在机器学习过程中, 对于函数 *dist(...)*, 若它是一"距离度量", 则应满足一些基本性质:  
    - 非负性: dist(Xi, Xj) >= 0.
    - 同一性: dist(Xi, Xj) = 0, 当且仅当 Xi = Xj.
    - 对称性: dist(Xi, Xj) = dist(Xj, Xi).
    - 直递性: dist(Xi, Xj) <= dist(Xi, Xk) + dist(Xk, Xj).

- 常见距离公式:  
    - 欧式距离: 空间中两点之间的距离, 即可用勾股定理计算.  
    - 曼哈顿距离: 只沿坐标轴方向计算距离: d = |Xi - Xj| + |Yi - Yj| ...  
    - 切比雪夫距离: 一点(整数坐标点)到相邻八个点的距离为1.  
    - 闵可夫斯基距离: 闵氏距离, 通过公式来从以上三个选择其一.

### K值选择

- k值过小: 容易受到异常点的影响, 容易过拟合.  
- k值过大: 受到样本均衡的影响, 容易欠拟合.  

《统计学习方法》-李航  

### kd树

实现k近邻算法时, 会使用kd树(平衡二叉树)存储点之间的距离信息, 否则每次都需要重新计算, 非常耗时.  
实现案例: [kd树的构造与查找](https://www.bilibili.com/video/BV1pf4y1y7kw?p=68)  



