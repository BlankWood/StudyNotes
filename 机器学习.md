**机器学习(Machine Learning)**

机器学习是从数据中自动分析获得模型, 并利用模型对未知数据进行预测.  

- 机器学习是人工智能的一个实现途径.  
- 深度学习是机器学习的一个方法发展而来.  

# 介绍

## 机器学习工作流程

1. 获得数据  
一行数据称为一个样本, 一列称为一个特征, 有些数据有目标值(标签值), 有些没有.数据一般分出20%~30%用于测试.  

2. 数据基本处理  
对数据进行缺失值,去除异常值等处理.  

3. 特征工程  
(专业的数据处理, 机器对特征的识别直接影响机器学习的效果以及上限), 包括:  
    - 特征提取
    - 特征预处理
    - 特征降维

4. 机器学习(模型训练)  
5. 模型评估  

## 人工智能主要分支

- 计算机视觉
- 语音识别
- 文本挖掘/分类
- 机器翻译
- 机器人

## 机器学习算法

- 监督学习

输入数据由输入特征值和目标值组成, 输出可以是一个连续的值(回归), 或是有限个离散的值(分类).  

    - 回归问题: 数据可由函数表示.
    - 分类问题: 数据可分类.

- 无监督学习

输入数据由特征值组成, 没有目标值.

- 半监督学习
- 强化学习 

## 模型评估

### 分类模型评估
准确率, 精确率, 召回率, F1-score, AUC指标等.  
主要学习准确率.  

### 回归模型评估
均方根误差(RMSE)是一个衡量回归模型误差率的常用格式.

### 拟合

- 欠拟合: 模型学习过于粗糙, 对数据特征关系要求过低.
- 过拟合: 对数据特征要求过高

## 深度学习

深度学习也称为深度结构学习, 层次学习, 深度机器学习. 基于神经网络发展而来...  

# 常用科学计算库

## matplotlib模块

一个常用的 python 2D 绘图库(元老级的).  
很多它python绘图库都是基于matplotlib开发的, 如seaborn, ggplot等.  

- matplotlib 三层结构:
    - 容器层: 由 Canvas(画板), Figure(画布), Axes(绘图区/坐标系)组成.  
        Canvas位于最底层, 充当画板, 用来放置Figure.  
        可以用 plt.subplots() 将画布分割成多个绘图区.  
    - 辅助显示层: 
        放置各种标题, 网格, 图例等图像信息.  
    - 图像层:  
        即各种绘制的图像.  

| 方法 | 说明 |
| --- | --- |
| figure(figsize=(), dpi=) | 创建画布, figsize为图的长宽, fig为清晰度 |
| plot(x:list, y:list) | 绘制折线 |
| scatter() | 绘制散点图 |
| bar() | 绘制柱状图 |
| hist(x, bins) | 绘制直方图, bins为组距 |
| pie() | 绘制饼图 |
| show() | 显示图像 |
| xticks(list) | 修改x轴刻度 |
| grid(True, linestyle='--', alpha=0.5) | 添加网格线, '--'表示虚线, alpha为透明度(0-1) |
| xlabel(str) | 添加x轴描述信息 |
| title(str) | 添加标题 |
| savefig(str) | 保存图像, 参数为保存路径, 需要在show()之前 |
| legend(loc="best") | 显示图例, 参数为位置 |


### 折线图

- pyplot模块:  
    其包含一系列类似于matlab的画图函数.  
    作用于当前图形(figure)的当前坐标系(axes).  
    `import  matplotlib.pyplot as plt`  


```python
# 图像中中文字体无法显示, 可修改配置文件或暂时修改配置:
from pylab import mpl
# 设置显示中文字体, 字体需要自行下载并安装, "SimHei"为字体名.  
mpl.rcParams["font.sans-serif"] = ["SimHei"]
# 防止部分字符无法正常显示
mpl.rcParams["axes.unicode_minus"] = False

import matplotlib.pyplot as plt
# 创建画布
plt.figure(figsize=(20, 8), dpi=100)

# 绘制图像
# 获取x,y轴数据
x = range(60)
y_temper = [random.uniform(15, 18) for i in x]

# 添加x,y轴
# 多次plot可获得多条折线
plt.plot(x, y_temper, color='r', label='G市')

# 获取x, y轴刻度
x_ticks_label = ["11点{}分".format(i) for i in x]
y_ticks = range(40)
# 修改x, y轴刻度
# 坐标刻度不可以直接通过字符串修改
# plt.xticks(x_ticks_label[::5])
plt.xticks(x[::5], x_ticks_label[::5])
plt.yticks(y_ticks[::5])

# 网格
plt.grid(linestyle="--", alpha=0.5)

# 添加描述信息
plt.title("某时段的天气温度变化")
plt.xlabel("时间")
plt.ylabel("温度")

# 图例, 需要先在折线添加label属性
plt.legend()

# 图像显示
plt.show()
```

- 多个绘图区:  
    figure, axes = plt.subplots(nrows=1, ncols=1, figuresize...)  
    参数为行数与列数.  
    返回值为画布, 坐标. 画布为共用.  
    用 aese[row][col] 代替 plt 表示画布.  
    axes[0][0].plot()


### 其他图表

- 散点图:  
    ```
    x = [x1, x2, x3...]
    y = [y1, y2, y3...]

    plt.scatter(x, y)
    ```

- 柱状图:  
    plt.bar(x, y)  
    当在同一坐标系中在画出柱状图时, 如果 x轴相同, 后者会覆盖前者.  
    通过改变 x轴, 可以控制柱子的位置.  

- 分布直方图:
    plt.hist(data, bins, density=False)  
    - 数据, 分组数, density: 显示为频率.

- 饼图:  
    plt.pie(data, labels, autopct, color)  
    - 数据, 各组的名称, 占比显示指定(%1.2f%%), 各组颜色  



## Numpy

Numpy是一个开源的Python科学计算库, 用于快速处理任意维度的数组.  
Numpy支持常见的数组和矩阵操作.  
Numpy使用ndarray对象来处理多维数组,该对象是一个快速而灵活的大数据容器.  

- ndarray属性:  
    - shape: 形状, 表示array各维度的长度.  
    - dtype: 数据类型, 其元素的数据类型.  


| 方法 | 说明 |
| --- | --- |
| numpy.array(list) | 返回n维数组, 参数为n维的数组 |
| ones/zeros([n, m]) | 生成n行m列的1/0数组 |
| ones/zeros_like(array) | 生成大小与array相同的n维1/0数组 |
| linspace(start, stop, num, endpoint) | 生成等差数列, num为元素个数(默认50), endpoint表示是否包含stop值(默认ture) |
| arange(start, stop, step, dtype) | 创建等差数组, step为步长(默认1) |
| logspace(start, stop, num) | 创建等比数列, num为元素个数(默认50) |
| random.normal(loc, scale, size) | 生成size个正态分布的随机数, 均值为loc, 标准差为scale |
| random.uniform(low, high, size) | 生成size个均匀分布的随机数, 范围: low-high |
| reshape(shape, order) | 返回一个具有相同数据域, 但shape不同的数组 |
| resize(new_shape) | 修改数组本身的形状 |
| T | 转置符, 调用后将矩阵转置 |
| astype(type) | 返回修改元素数据类型之后的数组, type为某数据类型 |
| tostring() | 序列化, 转换成bytes |
| unique(array) | 将传入的数组去重 |
| copy(ndarray) | 深拷贝 | 

- reshape(shape):  
    shape, 应为一个数组[x, y, z, ...], 表示各维度的元素个数.  
    如: [3, 4, 2], -1 表示不确定的个数, [-1, 5]表示行数不确定, 但只有2列.  
    有-1时, 如果形状不能完全填充就会报错.  

- 切片:  
    多维数组也可以进行切片, narray[n:m, x:y], 按维度一个一个切, 中间用 "," 隔开.  


### ndarray运算

1. 逻辑运算:  
    ```python
    import numpy as np

    # 生成10行5列, 40-100之间的随机整数
    score = np.random.randint(40, 100, (10, 5))

    # 切出后四行数据用于测试
    test_score = score[6: , 0:5]

    test_score[test_score > 60] = 1
    # test_score > 60, 此表达式会返回其结果的bool数组.  
    # ndarray 可以根据bool索引进行访问(访问其值为True的部分)
    ```

2. 通用判断函数:  
    - all(): 判断元素是否全部符合条件.
    - any(): 判断是否存在符合条件的元素.
    - 参数为bool数组.  

    ```
    # 数据接第一部分
    # 切取前两名学生的成绩
    np.all(score[0:2, :] > 60)

    np.any(score[0:2, :] > 90)
    ```

3. np.where(三元运算符):  
    - 参数1: 一个bool数组.  
    - 参数2: 将其中True修改为的值.  
    - 参数3: 将其中False修改为的值.  

    ```
    temp = score[:4, :4]

    np.where(temp > 60, 1, 0)

    np.where(np.logical_and(temp > 60, temp < 90), 1, 0)

    np.where(np.logical_or(temp > 90, temp < 60), 1, 0)
    ```

4. 统计运算

    | 运算 | 说明 |
    | --- | --- |
    | min(a, axis) | 返回最小值 |
    | max(a, axis) | 返回最大值 |
    | median(a, axis) | 计算中位数 |
    | mean(a, axis, dtype) | 计算平均值 |
    | std(a, axis, dtype) | 计算标准差 |
    | var(a, axis, dtype) | 计算方差 |
    | argmin/argmax(a, axis) | 返回最小/大值对应的下标 |

    - a为数组
    - axis: int 为行或列, 会计算每行(列)的统计值等. 可以为空, 此时统计范围为全部数据.  
    - 另一种调用方式: a.min(axis)

### 数组间的运算

1. 数组与数的运算
    ```
    a = np.array([[1,2,3], [3,4,5]])

    a + 3   # 返回全部元素 +3 后的数组
    a / 2   # 返回全部元素 /2 后的数组

    a = [1, 2, 3]   # 列表

    a * 3   # 列表会复制成3份, [1,2,3,1,2,3,1,2,3]
    ```

2. 数组与数组的运算
    同位置的元素进行运算.  

    不同形状数组之间的相加会通过**广播机制**进行扩展, 使两数组形状相同.  
    两数组应满足:  
    - 维度不同, 低维数组前扩展维度, 长度为1, 使其维度相同.  
    - 维度相同, 对应维度长度相同, 或其一为1.  
    例如: (2, 3) 可以看作 (1, 2, 3), (1, 1, 2, 3), (1, 1, 1, 2, 3). 只要相同维度的长度相同或其中一个为 1 即可进行运算.  


### 矩阵与向量

矩阵可以看作二维数组, 向量为 n x 1 的数组.  
矩阵可以用 array() 或 mat() 储存.  

「请」学习《线性代数》.  

矩阵乘法: np.matmul(a, b), np.dot()  
- matmul中不支持矩阵与标量相乘.  


### 合并与分割

- hstack((array1, array2)):  
    水平拼接: 将同行的元素拼接至同行.  

- vstack((array1, array2)):  
    竖直拼接: 将同列的元素拼接至同列.  

- concatenate((a1, a2), axis=):  
    axis: 0, 按行拼接; 1, 按列拼接.


### I/O与数据处理

详见pandas中.  
[DataFrame缺失值处理](#缺失值处理)


## Pandas

### 简介

- 专门用于数据挖掘的开源python库
- 以Numpy为基础
- 基于matplotlib, 能够简便画图
- 独特的数据结构

### pandas的数据结构

#### series(一维数组)

- 创建: pandas.Series(data, index= , dtype= )
    - data也可以为字典, 此时索引即为对应的key.  
    - index: 索引, 必须唯一, 且与数据长度等长. 如果没有传入则自动创建0-N的整数索引.  
    - dtype: 数据的类型, 可省略.  
- 属性(可直接访问):  
    - index: 索引
    - values: 值
    还可以通过 <series数组名>[下标] 的方式访问对应的值.  

#### DataFrame(二维数组)

- 创建: pd.DataFrame(data, index= , columns= )
    - index: 行标签, 可以为string列表, 默认0-N.  
    - columns: 列标签, 可以为string列表, 默认0-N.  
- 属性: shape, index, columns, value...    
    - T: 转置
    - head(n): 获取前n行数据, n默认为5.  
    - tail(n): 获取后n行数据, n默认为5.  
- 设置索引: <DataFrame对象名>.index/columns = list.  
    - 重置索引方法: reset_index(drop=True)  drop默认为false.
    - 以某列为索引: set_index(key, drop=True)  drop默认为True.

#### MultiIndex与Panel

MultiIndex: 多级索引(层次化索引), 是pandas的重要功能. Series, Data Frame对象内也有MultiIndex对象.  

- 创建: pd.MultiIndex.from_arrays(arrays, name= )
    - arrays: 索引列表, 二维string数组.
    - name: string数组, 代表每个索引的名字.
- 属性:  
    - names: 索引的名字
    - levels: 索引(有序列表)
    - labels: 索引对应的顺序

拥有三个索引的数组就是三维数组, panel为储存三维数组的容器.  

- 创建: pandas.Panel(data, items, major_axis, minor_axis)
    - 参数为数据与三个索引

无法直接查看, 只能先切成二维数组再查看.  

### 基本数据操作

#### 索引操作

1. <容器名>[列下标][行下标] 或 <容器>.下标 获取对应元素.  
2. <容器>.loc["索引1" : "索引2", ...] : 获取从索引1到索引2的数据, 索引值.  
3. <容器>.iloc[x1 : x2, ...] : 获取第x1到第x2的数据, 索引下标.  

#### 排序

- sort_values(by= , ascending= ) : 根据值排序
    - by: 排序依据的索引(依据哪个索引排序), 可以为多个.  
    - ascending: 默认为True(升序), False为降序.  

- sort_index() : 按索引值排序.  

### DataFrame运算

- 算术运算  
    <容器>[索引值].add(...) 或 <容器>[索引值] + ...(不推荐)  

- 逻辑运算  
    <容器>[索引值] > ...  
    <容器>.query(expr:string) : expr为查询字符串, 形式为: "索引值 > 60 & 索引值 < 80 ", 此时此索引下的值大于60且小于80的会返回true.  
    <容器>[索引值].isin(values) : 判断此索引下的数据是否再某个范围之间, values为有两个元素的list.

- 统计运算
    - describe(): 综合分析, 能够直接得出count, mean, std, min, max等
    - 个单独的统计函数, sum, mean等, 具体可参考Numpy.  

- 自定义运算: apply(fun, axis=0)
    - fun: 自定义函数
    - axis: 默认(0)为列, 1为行

### Pandas画图

Series/DataFrame.plot(kind), Series或DataFrame对象可直接调用plot进行绘制图像, 参数为图像的种类, 默认为"line(折线图)".  

### 文件读取与存储

pandas支持多种文件格式的读取, 包括 CSV, Json, HTML, Lacol, HDF等.  

#### CSV(.csv)

- pandas.read_csv(filepath_or_buffer, sep=',')
    - filepath_or_buffer: 文件路径.  
    - usecols: 指定读取的列名, 列表形式.
- DataFrame.to_csv(path_or_buf, sep=',', columns=None, header=True, index=True, mode='w')  
    - sep: 分隔符,默认为','.
    - columns: 需要写入的列索引.
    - header: 是否写入列索引值, 默认True.
    - index: 是否写入行索引值.
    - mode: 'w'为写入, 'a'为追加.

#### HDF5(.h5)

- pandas.read_hdf(path_or_buf, key=None) 
- DataFrame.to_hdf(path_or_buf, key)

HDF5文件的读取和存储需要指定一个键, 值为要存储的DataFrame.  
优先选择使用HDF5文件存储.  
1. HDF5在存储时支持压缩, 使用的方式为blosc, 这个是速度最快的也是pandas默>支持的.  
2. HDF5支持跨平台, 可以轻松迁移到Hadoop上面.  

#### Json(.json)

- read_json(path_or_buf, orient, typ='frame', lines=False)
    - 将JSON格式转化成默认的Pandas DataFrame格式.  
    - orient: 数据的显示形式  
        - split: 行索引, 列索引, 数据分开.
        - records: 以 columns : values 的形式输出.  
        - index: 以 index : {columns : values} 的形式输出.  
        - columns: 以 columns : {index : values} 的形式输出.  
        - values: 直接输出值
    - typ: 转换成的数据为dataframe或series.  
    - lines: 按照行读取.  

- to_json(path, orient, lines)

### 高级处理

#### 缺失值处理

1. 缺失值标记为NaN

    - 判断  
        - pd.isnull(df)  
        - pd.notnull(df)
        - 一般配合np.all()与np.any()使用.  
    - 处理  
        - 删除: dropna(axis='rows')  
            - 不会修改原数据, 需要接收返回值  
        - 替换: fillna(value, inplace=True)  
            - values: 替换值  
            - inplace: 是否修改原数据, false则返回新数据  

2. 不是NaN, 例如"?"  
    
    - 先替换为np.nan, 然后继续处理.  
    - df.replace(to_replace='?', value=np.nan)

#### 数据离散化

- 目的：  
连续属性离散化的目的是为了简化数据结构, 数据离散化技术可以用来减少给定连续属性值的个数。离散化方法经常作为数据挖掘的工具。  

- 具体内容：  
连续属性的离散化就是在连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数值代表落在每个子区间中的属性值。  
例如：有一组身高数据(连续属性)，将身高划分为几个区间：150~165、165~180，180~195，并标记为矮、中、高三个类别，最终要处理成一个"哑变量"矩阵。  
也就是用矮、中、高代替身高数据。  

- 方法:  
    - pd.qcut(data, q)  # 将数据自动分成数量差不多的q组  
    - series.value_counts()    # 统计分组数据数量  
    - pd.cut(data, bins)    # bins为自己指定的分组区间, 例:[0,5,10]  

将分组可以转化为表格: 横轴为各区间, 纵轴为数据下标, 值为1表示在此区间.  
- pd.get_dummies(p_count, prefix:string)   # p_count为分好的组, prefix为区间的前缀取名.  

#### 合并

- pd.concat([data1, data2], axis=1)  
    将两表按照行或列进行合并, axis=0(默认)为列索引, 1为行索引.  

- pd.merge(left, right, how="inner", on=None)  
    - 可以指定按照两组数据的共同键值对合并或者左右各自  
    - left: DataFrame  
    - right: 另一个DataFrame  
    - on: 指定的共同键
    - how: 按照什么方式连接, 默认内连接  

| Merge method | SQL Join Name |
| --- | --- |
| left | 左连接 |
| right | 右连接 |
| outer | 外连接 |
| inner | 内连接 |

#### 交叉表与透视表

交叉表：用于计算一列数据对于另一列数据的分组个数(用于统计分组频率的特殊透视表), 通常还会借助绘图直观地显示.  
例: 一列"性别(男女)"对另一列"分数及格"之间的统计关系. 使用前应先处理分数数据, 将其转换为"及格(1)"或"不及格(0)".  
- pd.crosstab(value1, value2)

透视表: 将原有的DataFrame的列分别作为行索引和列索引, 然后对指定的列应用聚集函数.  
- data.pivot_table()  
- DataFrame.pivot_table([], index)  # 前参数为被统计的列表, 后者为统计的列作为索引.  

#### 分组与聚合

将数据分组并统计个数.  

分组: `dataframe.groupby(key, as_index=False)`  
    - key: 分组的列数据, 可以多个.  
    - as_index: 是否将分组的列作为索引, 默认True.  

```
col = pd.DtaFrame({'color':['white', 'red', 'green', 'red', 'green'], 'object':['pen', ...], ...})

| index | color | object | price |
|   0   | white | pen    | 5.56  |
|   1   | red   | pencil | 4.20  |
|   2   | green | pencil | 3.20  |
|   3   | red   | ashtray| 0.56  |
|   4   | green | pen    | 2.75  |

col.groupby(['color'])['price'].mean()
col['price'].groupby(col['color']).mean()

```

## 电影数据案例分析

### 需求

一组最近10年1000部最流行的电影数据.  
数据来源: https://www.kaggle.com/damianpanek/sunday-eda/data  

问题1: 我们想知道这些电影数据中评分的平均分, 导演的人数等信息, 该如何获取.  
问题2: 对于这一组电影数据, 如果我们想rating, runtime的分布情况, 应该如何呈现数据.  
问题3: 对于这一组电影数据, 如果我们希望统计电影分类(genre)的情况, 应该如何处理数据.  

### 实现

导入包, 获取数据:  
```
%matplotlib inline  # 解决有些平台matplotlib无法显示
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

# 文件路径
path = "./data/IMDB-Movie-Data.csv"

# 读取文件
df = pd.read_csv(path)
```

1. 问题1:  
- 评分的平均分: `df["Rating"].mean()`  
- 导演的人数: 求出唯一值, 再根据其形状得出.  
    `np.unique(df["Director"]).shape[0]     # df["Director"].unique().shape[0]`  

2. 问题2:  
- Rating进行分布展示  
```
# 创建画布
plt.figure(figsize=(20, 8), dpi=80)
# 绘制图像
plt.hist(movie["Rating"].values, bins=20)

# 添加刻度
max_ = move["Rating"].max()
min_ = move["Rating"].min()

t1 = np.linspace(min_, max_, num=21)

plt.xticks(t1)

# 添加网格
plt.grid()

# 显示
plt.show()
```

- Runtime (Minutes)分布, 与Rating相似.  
```
# 创建画布
plt.figure(figsize=(20, 8), dpi=80)
# 绘制图像
plt.hist(movie["Runtime (Minutes)"].values, bins=20)

# 添加刻度
max_ = move["Runtime (Minutes)"].max()
min_ = move["Runtime (Minutes)"].min()

t1 = np.linspace(min_, max_, num=21)

plt.xticks(t1)

# 添加网格
plt.grid()

# 显示
plt.show()
```

3. 问题3:  
思路: 制作一张统计表, 一行为一部电影, 列为电影的类别, 对应的行列标记为 1. 最后对每一列求和. 或者统计时直接加上.  

```python
# 获取所有的电影类别
# 一部电影可能有多个类别, 其用","隔开
temp_list = [i.split(",") for i in movie["Genre"]]

# 此时temp_list内的元素为list, 每个list代表一部电影的类别
# 继续分割, 将所有类别放在一个list中, 并去重
genre_list = np.unique([i for j in temp_list for i in j])

# 获取 电影个数 x 类别个数 大小的0数组
zeros = np.zeros([movie.shape[0], genre_list.shape[0]])

temp_movie = pd.DataFrame(zeros, columns=genre_list)

for i in range(1000)
    temp_movie.ix[i, temp_list[i]] = 1

genre = temp_movie.sum().sort_values(ascending=False)

# 绘图
genre.plot(kind="bar", figsize=(20, 8), fontsize=16, colormap="cool")
```

## seaborn绘图

Matplotlib虽然已经是比较优秀的绘图库了, 但是它有个令人头疼的问题, 那就是API使用过于复杂, 它里面有上千个函数和参数, 属于典型的那种可以用它做任何事, 却无从下手.  
Seaborn基于Matplotlib核心库进行了更高级的API封装, 可以轻松地画出更漂亮的图形, 而Seaborn的漂亮主要体现在配色更加舒服, 以及图形元素的样式更加细腻.  

导入包(先安装): `import seaborn as sns`  

### 绘制单变量分布
  
seaborn.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None, color=None)  
    - a:    表示要观察的数据, 可以是Series, 一维数组或列表.  
    - bins: 用于控制条形的数量.
    - hist: 表示是否绘制(标注)直方图.
    - kde:  表示是否绘制高斯核密度估计曲线.
    - rug:  表示是否在支持的轴方向上绘制rugplot.

### 绘制双变量分布

jointplot为双变量分布图表接口. 中心为散点图, 上侧与右侧为量变量的直方图.  

jointplot(x, y, data=None, kind="scatter", stat_func=None, color=None, ratio=5, space=0.2, dropna=True)  
    - kind:     表示绘制图形的类型.
    - stat_func:用于计算有关关系的统计量并标注图.
    - color:    表示绘图元素的颜色.
    - size:     用于设置图的大小(正方形).
    - ratio:    表示中心图与侧边图的比例. 该参数与中心图的占比成正比.
    - space:    用于设置中心图与侧边图的间隔大小.

二维直方图:  
通过颜色深浅表示其区域内数据的计数, 适用于较大的数据集.  
sns.jointplot(x="x", y="y", data, kind="hex")  

核密度估计图形:  
sns.jointplot(x, y, data, kind="kde")

绘制成对的双变量分布:  
想要在数据集中绘制多个成对的双变量分布, 则可以使用pairplot()实现, 该函数会创建一个坐标轴矩阵, 并且显示Dataframe对象每对变量的关系. 另外, pairplot()函数也可以绘制每个变量在对角轴上的单变量分布.  

```
# iris(鸢尾花), 这是sns自带的一个数据集, 载入用于测试
dataset = sns.load_dataset("iris")
# 绘图
sns.pairplot(dataset)
```

### 用分类数据绘图

#### 类别散点图

sns.stripplot(x, y, hue, data, order, hue_order, jitter=False)
    - x, y, hue: 用于绘制长格式数据的输入.
    - data:      用于绘制的数据集, 如果x, y不存在, 则它将作为宽格式, 否则作为长格式.
    - jitter:    表示抖动的程度(仅沿类别轴). 当很多数据点重叠时, 可以指定抖动的数量或者设为True使用默认值.

使用swarmplot(), 数据将不会重叠.  

```python
tips = sns.load_dataset("tips")

sns.stripplot(x="day", y="total_bill", data=tips)
# sns.stripplot(x="day", y="total_bill", data=tips, jitter=True)
```

具体的演示: https://www.bilibili.com/video/BV1pf4y1y7kw?p=51  

#### 类别内的数据分布

- 箱形图  
    箱形图(Box-plot), 又称为盒须图、盒式图或箱线图, 是一种用作显示一组数据分散情况资料的统计图. 因形状如箱子而得名.  
    它能显示出一组数据的最大值, 最小值, 中位数, 及上下四分位数.  

seaborn.boxplot(x, y, hue, data, orient, color, saturation=0.75, width=0.8)  
    - palette:  用于设置不同级别色相的颜色("r","g","b","y").  
    - saturation:  用于设置数据显示的颜色饱和度.  


- 小提琴图  
    用于显示数据分布及其概率密度.  
    这种图表结合了箱形图和密度图的特征, 主要用来显示数据的分布形状.  

seaborn.violinplot(x, y, hue, data)

#### 类别内的统计估计

- 条形图:  
sns.barplot(x, y, data)

- 点图:  
sns.pointplot(x, y, data)

### 相关性基本分析

通过**热力图**分析两列数据之间的相关性。

`dataframe.corr()    # 返回容器中每两列数据之间的相关性表格.`  

将相关性表格数据绘制成热力图, 通过颜色深浅直观地显示相关性大小.  
sns.heatmap(coor_name, square=False, linewidth, annot=False)
    - corr_name:  获取的相关性数据变量名.
    - square:     是否为方形.
    - linewidth:  方格之间的空隙大小, 默认0.
    - annot:      显示具体的数值.

调整图的大小: `plt.figure(figsize=(20, 8), dpi=100)`


### 综合案例-北京租房数据




### 鸢尾花数据集

iris(鸢尾花)数据集时常用的分类实验数据集, 由Fisher1936年收集整理, 时一类多重变量分析的数据集.  

- 特征值4个: 花瓣、花萼的长度、宽度  
- 目标值3个: 三种鸢尾花种类(山鸢尾, 虹膜锦葵, 变色鸢尾)  

#### API

sklearn.datasets

- datasets.load_*(): 获取小规模数据集, 数据包含在datasets里.  
- datasets.fetch_*(data_home): 获取大规模数据集, 需要从网络上下载. 参数为目录(默认为 ~/scikit_learn_data/)

获取鸢尾花数据集:  
`iris = sklearn.datasets.load_iris()`

返回的数据类型为 datasets.base.Bunch(字典格式)  
    - data: 特征数据数组, 二维numpy.ndarray数组.  
    - target: 标签(目标)数组, 一维numpy.ndarray数组.  
    - DESCR: 数据描述.  
    - feature_names: 特征名, 新闻数据、手写数字、回归数据集没有.  
    - target_names: 标签(目标)名.  

# 算法

## Scikit-Learn

Scikit-learn为python的机器学习工具, 包含许多知名的机器学习算法的实现.  
Scikit-learn文档完善, 容易上手, 有丰富的API. 稳定的版本: 0.19.1  

### 安装

`pip install scikit-learn==0.19.1`  
安装需要Numpy, Scipy等库.  
`import sklearn`  


## 线性回归



### 代价函数

通过数据集来得到模型, 为了测量(评估)模型是否拟合数据, 以代价函数作为衡量标准.  

在线性回归问题中, 代价函数一般为方差:  
$$ \frac{1}{2m} \sum_{i=1}^{m}(h_θ(x_i)-y_i)^2 $$  
    m为数据集的个数, hθ为模型(回归函数, 比如 y=θ1x+θ2).  
在线性回归问题中用待定系数法确定模型的系数, 通过代价函数将问题转换为求各系数下代价函数的最小值, 以此确定系数.  
一般表示为: J(...).  

当模型(回归函数)中的系数取不同值时, 会得到不同的方差, 构成自变量为回归函数系数, 因变量为对应方差的函数. 根据此函数得到方差最小值时, 此时的系数(自变量)即模型最拟合的系数取值.  

代价函数一般会选择凹函数(局部最小为全局最小), 避免选择的局部最小与全局最小差距过大, 模型不合理.  

### 梯度下降算法

梯度下降算法常用来计算最小的方差等.  
此算法将从系数的某个取值(一般全赋0)开始, 将θ不断朝导数为负的方向移动, 直到(区域)最低处.  

$temp0 := θ_0 - α \frac{d}{dθ_0}J(θ_0, θ_1)$  
$temp1 := θ_1 - α \frac{d}{dθ_1}J(θ_0, θ_1)$  
$θ_0 := temp0$  
$θ_1 := temp1$  
- J()即计算根据系数求方差的函数. α决定每次跨步的大小.  

### 多变量线性回归

通常用多维向量来表示多元变量, 回归函数就变成:  
$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + ... +θ_nx_n$.  
变量θ, x用向量表示:  
$h_θ(x) = θ^TX$, (θ, X为n维向量)

### 多元梯度下降法

与单变量类似, 导数为每个θ变量的偏导, 不断更新θ向量.  

#### 特征收缩

在多个变量的梯度下降中, 由于变量的取值范围差异较大, 导致下降时的路线会非常曲折, 效率低下. 因此需要通过缩放来使变量范围大致相同, 一般在 0~1, -1~1, 0~3之类.  

#### 学习率

在梯度下降中, 每次下降(迭代)都会使 θ 接近最小值, 随迭代此数不断下降.  
以迭代次数为自变量, θ高度为因变量, 会得到类似 y = 1/x 的函数, 即 θ 随迭代次数增加会收敛.  
可以通过绘制这个函数来判断梯度下降算法是否收敛.  

自动收敛测试:  
如果代价函数J(θ)进一步迭代后的下降小于一个很小的值(如e^-3), 就判断函数已经收敛.  
但要设定一个合适的阈值比较困难, 更建议看曲线图.  

也可以通过曲线图判断梯度下降算法是否正常工作(α过大, 或程序错误之类).  

### 正规方程

在求回归函数时, 之前时是通过不断迭代求θ向量, 直到代价函数最小值. 但可以通过运算来直接求代价函数最小值的位置.  

正规方程:  
$θ = (X^T X)^{-1} X^T y$  

X为特征矩阵, y为目标向量, X一般会在第一列加全为1的一列作为常数项.  
$(X^T X)$ 是为了得到方阵, 方阵才有逆矩阵.  
计算时间(求解θ)一般为O(n^3), n为X的维度.  
目前的计算机在n > 10000时不建议使用正规方程, 而使用梯度下降.

#### 矩阵不可逆时求解

一般的特征矩阵不会出现不可逆的情况, 特征之间一般是线性无关的.  
即使矩阵不可逆, pinv()函数也可以通过伪逆矩阵求解.  

原因:  
- 在特征值中有多余的特征变量(或者重复之类的)
- 特征变量过多而样本太少

## 分类

### 二(元)分类

#### 逻辑回归(logistic regression)

虽然带名字有回归, 但是个分类算法.  
对于求解分类的函数(假设函数):  
$h_θ(x) = g(θ^T x)$  
$g(z) = \frac{1}{1 + e^{-z}}$  
函数g(z)能将结果控制在 0~1 之间. 或是将其转化为概率.

通过训练后能得到模型θ向量, 与特征变量构成一个界限(坐标中的几何曲线或面)  
这曲线就是决策界限(分类界线), 将其与g(z)复合, 就能得到0或1的分类判断.  

#### 代价函数

代价函数可以写成:  
$$J(θ) = \frac{1}{m} \sum^{m}_{i=1} Cost()$$

$$ Cost(h_θ(x), y) = \left\{
\begin{aligned}
    -log(h_θ(x)) \quad if & & y = 1  \\
-log(1 - h_θ(x)) \quad if & & y = 0 \\
\end{aligned}
\right.
$$

预测值 $h_θ(x)$ 在 0-1 内.  
如果预测值与其类别差距越大, 则代价函数的值就越大.  

简化:  
$Cost(h_θ(x), y) = -y log(h_θ(x)) - (1-y)log(1-h_θ(x))$  





#### K-近邻算法

K Nearest Neighbor(KNN算法). 一种分类算法.  
如果一个样本在特征空间中的**k个最相似(即特征空间中最邻近)的样本中大多数属于某一个类别**. 则该样本也属于这个类别.  

距离公式: 根据数据与勾股定理计算距离.  

将一样本分类时, 选择离它 "最近的" k个样本, 它属于k个样本中大多数的那个类别.

##### K-邻近算法API

sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)
    - n_neighbor: 查询使用的邻居数, 默认为5.  


```python
from sklearn.neighbors import KNeighborsClassifier

# 1.构造数据
x = [[1], [2], [10], [20]]
y = [0, 0, 1, 1]

# 2.训练模型
# 2.1 实例化一个估计器对象
estimator = KNeighborsClassifier(n_neighbors=1)

# 2.2 调用fit方法, 进行训练
estimator.fit(x, y)

# 3.数据预测
ret = estimator.predict([[0]])
print(ret)
```

##### 距离度量

- 在机器学习过程中, 对于函数 *dist(...)*, 若它是一"距离度量", 则应满足一些基本性质:  
    - 非负性: dist(Xi, Xj) >= 0.
    - 同一性: dist(Xi, Xj) = 0, 当且仅当 Xi = Xj.
    - 对称性: dist(Xi, Xj) = dist(Xj, Xi).
    - 直递性: dist(Xi, Xj) <= dist(Xi, Xk) + dist(Xk, Xj).

- 常见距离公式:  
    - 欧式距离: 空间中两点之间的距离, 即可用勾股定理计算.  
    - 曼哈顿距离: 只沿坐标轴方向计算距离: d = |Xi - Xj| + |Yi - Yj| ...  
    - 切比雪夫距离: 一点(整数坐标点)到相邻八个点的距离为1.  
    - 闵可夫斯基距离: 闵氏距离, 通过公式来从以上三个选择其一.

##### K值选择

- k值过小: 容易受到异常点的影响, 容易过拟合.  
- k值过大: 受到样本均衡的影响, 容易欠拟合.  

《统计学习方法》-李航  

##### kd树

实现k近邻算法时, 会使用kd树(平衡二叉树)存储点之间的距离信息, 否则每次都需要重新计算, 非常耗时.  
实现案例: [kd树的构造与查找](https://www.bilibili.com/video/BV1pf4y1y7kw?p=68)  



